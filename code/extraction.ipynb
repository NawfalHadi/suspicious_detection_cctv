{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88f41506",
   "metadata": {},
   "outputs": [],
   "source": [
    "ip_cam = \"http://192.168.100.197:5000/video\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf9c43a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/c/users/nawfal/documents/apps/collaborative project/cctv_motion_detection/motion_venv/bin/python'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.executable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df3691d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ebb4d3",
   "metadata": {},
   "source": [
    "# Camera Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7267d9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Camera is ready. Press 'q' to quit.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the camera\n",
    "camera = cv2.VideoCapture(\"http://192.168.100.197:5000/video\")\n",
    "\n",
    "# Check if the camera opened successfully\n",
    "if not camera.isOpened():\n",
    "    print(\"Error: Could not open camera.\")\n",
    "else:\n",
    "    print(\"Camera is ready. Press 'q' to quit.\")\n",
    "\n",
    "# Capture frames from the camera\n",
    "while True:\n",
    "    ret, frame = camera.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to grab frame.\")\n",
    "        break\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.namedWindow(\"Camera Capture\", cv2.WINDOW_NORMAL | cv2.WINDOW_GUI_NORMAL)\n",
    "    cv2.resizeWindow(\"Camera Capture\", 640, 480)\n",
    "    cv2.imshow(\"Camera Capture\", frame)\n",
    "\n",
    "    # Exit the loop when 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the camera and close all OpenCV windows\n",
    "camera.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b290bf",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtime\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m num_frames = \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mHow many frames do you want? \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m frames = []\n\u001b[32m      6\u001b[39m camera = cv2.VideoCapture(\u001b[33m\"\u001b[39m\u001b[33mhttp://192.168.50.234:5000/video\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: invalid literal for int() with base 10: ''"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "num_frames = int(input(\"How many frames do you want? \"))\n",
    "frames = []\n",
    "\n",
    "camera = cv2.VideoCapture(\"http://192.168.50.234:5000/video\")\n",
    "\n",
    "desire_fps = 10\n",
    "frame_delay = 1.0 / desire_fps\n",
    "\n",
    "# Check if the camera opened successfully\n",
    "if not camera.isOpened():\n",
    "    print(\"Error: Could not open camera.\")\n",
    "else:\n",
    "    print(\"Camera is ready. Press 'q' to quit.\")\n",
    "\n",
    "# Capture frames from the camera\n",
    "while True:\n",
    "    start_time = time.time()\n",
    "\n",
    "    ret, frame = camera.read()\n",
    "    \n",
    "    if ret:\n",
    "        # Save the frame to the test/frames folder\n",
    "        # Ensure the directory exists before saving\n",
    "        os.makedirs(\"test/frames\", exist_ok=True)\n",
    "        filename = f\"test/frames/frame_{len(frames)}.jpg\"\n",
    "        cv2.imwrite(filename, frame)\n",
    "\n",
    "        frames.append(frame)\n",
    "        print(len(frames))\n",
    "\n",
    "        if len(frames) >= num_frames:\n",
    "            print(f\"Saved {num_frames} frames.\")\n",
    "            break\n",
    "\n",
    "    if not ret:\n",
    "        print(\"Failed to grab frame.\")\n",
    "        break\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.namedWindow(\"Camera Capture\", cv2.WINDOW_NORMAL | cv2.WINDOW_GUI_NORMAL)\n",
    "    cv2.resizeWindow(\"Camera Capture\", 640, 480)\n",
    "    cv2.imshow(\"Camera Capture\", frame)q\n",
    "\n",
    "    # Delay to simulate target FPS\n",
    "    elapsed = time.time() - start_time\n",
    "    sleep_time = max(0, frame_delay - elapsed)\n",
    "    time.sleep(sleep_time)\n",
    "\n",
    "\n",
    "    # Exit the loop when 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the camera and close all OpenCV windows\n",
    "camera.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050ec453",
   "metadata": {},
   "source": [
    "# Get Continously Frame\n",
    "the output is the list always changed while camera running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd66ba90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Camera is ready. Press 'q' to quit.\n",
      "append\n",
      "1\n",
      "append\n",
      "2\n",
      "append\n",
      "3\n",
      "append\n",
      "4\n",
      "append\n",
      "5\n",
      "append\n",
      "6\n",
      "append\n",
      "7\n",
      "append\n",
      "8\n",
      "append\n",
      "9\n",
      "append\n",
      "10\n",
      "append\n",
      "11\n",
      "append\n",
      "12\n",
      "append\n",
      "13\n",
      "append\n",
      "14\n",
      "append\n",
      "15\n",
      "Saved 15 frames.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "num_frames = int(input(\"How many frames do you want? \"))\n",
    "frames = []\n",
    "\n",
    "camera = cv2.VideoCapture(\"http://192.168.50.234:5000/video\")\n",
    "\n",
    "desire_fps = 10\n",
    "frame_delay = 1.0 / desire_fps\n",
    "\n",
    "# Check if the camera opened successfully\n",
    "if not camera.isOpened():\n",
    "    print(\"Error: Could not open camera.\")\n",
    "else:\n",
    "    print(\"Camera is ready. Press 'q' to quit.\")\n",
    "\n",
    "# Capture frames from the camera\n",
    "while True:\n",
    "    start_time = time.time()\n",
    "\n",
    "    ret, frame = camera.read()\n",
    "    \n",
    "    if ret:\n",
    "        # Save the frame to the test/frames folder\n",
    "        # Ensure the directory exists before saving\n",
    "\n",
    "\n",
    "        # cek apakah list nya ada None\n",
    "        if len(frames) < num_frames:\n",
    "            frames.append(frame)\n",
    "            print(\"append\")\n",
    "        else:\n",
    "            # do a movement where the list work move from second index to the fist index\n",
    "            # third index to the second index and so on\n",
    "            # the first index will be deleted\n",
    "            # the last index will be added from the camera\n",
    "            frames.pop(0)\n",
    "            print(\"pop\")\n",
    "\n",
    "        print(len(frames))\n",
    "\n",
    "        if len(frames) >= num_frames:\n",
    "            print(f\"Saved {num_frames} frames.\")\n",
    "            break\n",
    "\n",
    "    if not ret:\n",
    "        print(\"Failed to grab frame.\")\n",
    "        break\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.namedWindow(\"Camera Capture\", cv2.WINDOW_NORMAL | cv2.WINDOW_GUI_NORMAL)\n",
    "    cv2.resizeWindow(\"Camera Capture\", 640, 480)\n",
    "    cv2.imshow(\"Camera Capture\", frame)\n",
    "\n",
    "    # Delay to simulate target FPS\n",
    "    elapsed = time.time() - start_time\n",
    "    sleep_time = max(0, frame_delay - elapsed)\n",
    "    time.sleep(sleep_time)\n",
    "\n",
    "\n",
    "    # Exit the loop when 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the camera and close all OpenCV windows\n",
    "camera.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25109664",
   "metadata": {},
   "source": [
    "# Show Testing GUI\n",
    "i want the output is to show all of FRAMEn while camera active\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d3559351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Camera is ready. Press 'q' to quit.\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "camera = cv2.VideoCapture(\"http://192.168.100.197:5000/video\")\n",
    "num_frames = 15\n",
    "\n",
    "original_width, original_height = 500, 500\n",
    "small_width, small_height = 150, 150\n",
    "\n",
    "default_frame = 255 * np.ones((small_height, small_width, 3), dtype=np.uint8)\n",
    "\n",
    "desire_fps = 20\n",
    "frame_delay = 1.0 / desire_fps\n",
    "\n",
    "frames = []\n",
    "\n",
    "# Check if the camera opened successfully\n",
    "if not camera.isOpened():\n",
    "    print(\"Error: Could not open camera.\")\n",
    "else:\n",
    "    print(\"Camera is ready. Press 'q' to quit.\")\n",
    "\n",
    "while True:\n",
    "    start_time = time.time()\n",
    "    ret, frame = camera.read()\n",
    "\n",
    "    if ret:\n",
    "        frame = cv2.resize(frame, (original_width, original_height))\n",
    "\n",
    "        if len(frames) < num_frames:\n",
    "            frames.append(cv2.resize(frame, (small_width, small_height)))\n",
    "        else:\n",
    "            frames.pop(0)\n",
    "            frames.append(cv2.resize(frame, (small_width, small_height)))\n",
    "        \n",
    "        print(len(frames))\n",
    "\n",
    "        # 3 rows vertically stacked\n",
    "\n",
    "        if len(frames) >= 15:\n",
    "            stacked1_frames = np.vstack((frames[0], frames[1], frames[2]))\n",
    "            stacked2_frames = np.vstack((frames[3], frames[4], frames[5]))\n",
    "            stacked3_frames = np.vstack((frames[6], frames[7], frames[8]))\n",
    "            stacked4_frames = np.vstack((frames[9], frames[10], frames[11]))\n",
    "            stacked5_frames = np.vstack((frames[12], frames[13], frames[14]))\n",
    "\n",
    "            # Resize to match width of original frame\n",
    "            stacked1_frames = cv2.resize(stacked1_frames, (small_width, original_height))\n",
    "            stacked2_frames = cv2.resize(stacked2_frames, (small_width, original_height))\n",
    "            stacked3_frames = cv2.resize(stacked3_frames, (small_width, original_height))\n",
    "            stacked4_frames = cv2.resize(stacked4_frames, (small_width, original_height))\n",
    "            stacked5_frames = cv2.resize(stacked5_frames, (small_width, original_height))\n",
    "\n",
    "            final_layout = np.hstack((frame, stacked1_frames, stacked2_frames, stacked3_frames, stacked4_frames, stacked5_frames))\n",
    "        else:\n",
    "            final_layout = frame  # or show default layout\n",
    "\n",
    "        \n",
    "        # Delay to simulate target FPS\n",
    "        elapsed = time.time() - start_time\n",
    "        sleep_time = max(0, frame_delay - elapsed)\n",
    "        time.sleep(sleep_time)\n",
    "\n",
    "\n",
    "        cv2.imshow(\"Camera Capture\", final_layout)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "camera.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56193f8a",
   "metadata": {},
   "source": [
    "# Implement Landmark On Running Frames "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3c593e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-04 17:12:49.062570: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1756977169.182957   42331 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1756977169.216548   42331 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1756977169.475770   42331 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1756977169.475856   42331 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1756977169.475861   42331 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1756977169.475864   42331 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-09-04 17:12:49.520590: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import mediapipe as mp\n",
    "from mediapipe.framework.formats import landmark_pb2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd3e54a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_drawing = mp.solutions.drawing_utils # Drawing helpers\n",
    "mp_holistic = mp.solutions.holistic # Mediapipe Solutions\n",
    "\n",
    "\" Offset value should be based on the size of player detection \"\n",
    "offset = 10 # temp value\n",
    "\n",
    "line_thickness = 2\n",
    "line_color = (0, 255, 0)  # Green color\n",
    "line_color_red = (0, 0, 255)  # Red color\n",
    "line_color_blue = (255, 0, 0)  # Blue color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f3b284c",
   "metadata": {},
   "outputs": [],
   "source": [
    "desire_fps = 5\n",
    "num_frames = 15\n",
    "\n",
    "original_width, original_height = 500, 500\n",
    "small_width, small_height = 150, 150"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e474a9",
   "metadata": {},
   "source": [
    "# Get Landmark of frames By Initialized "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fca14e6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Camera is ready. Press 'q' to quit.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MESA: error: ZINK: failed to choose pdev\n",
      "libEGL warning: egl: failed to create dri2 screen\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1756978535.872796   42331 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1756978535.928884   46723 gl_context.cc:369] GL version: 3.1 (OpenGL ES 3.1 Mesa 24.0.9-0ubuntu0.3), renderer: D3D12 (AMD Radeon(TM) Graphics)\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1756978536.448428   46701 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1756978536.756249   46706 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1756978536.776058   46709 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n",
      "W0000 00:00:1756978536.782099   46705 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1756978536.782281   46708 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1756978536.791424   46707 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1756978536.815112   46704 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1756978536.834011   46700 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1756978536.834695   46701 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "10\n",
      "15\n",
      "20\n",
      "25\n",
      "30\n",
      "35\n",
      "40\n",
      "45\n",
      "50\n",
      "55\n",
      "60\n",
      "65\n",
      "70\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n",
      "75\n"
     ]
    }
   ],
   "source": [
    "camera = cv2.VideoCapture(ip_cam)\n",
    "\n",
    "frames = []\n",
    "detected = []\n",
    "\n",
    "if not camera.isOpened():\n",
    "    print(\"Error: Could not open camera.\")\n",
    "else:\n",
    "    print(\"Camera is ready. Press 'q' to quit.\")\n",
    "\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while camera.isOpened:\n",
    "        start_time = time.time()\n",
    "        ret, frame = camera.read()\n",
    "        \n",
    "        if ret:\n",
    "            frame = cv2.resize(frame, (original_width, original_height))\n",
    "\n",
    "\n",
    "            if len(frames) < num_frames:\n",
    "                frames.append(cv2.resize(frame, (small_width, small_height)))\n",
    "                \n",
    "                frame_detected = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                frame_detected.flags.writeable = False\n",
    "                results = holistic.process(frame_detected)\n",
    "                frame_detected.flags.writeable = True\n",
    "                frame_detected = cv2.cvtColor(frame_detected, cv2.COLOR_RGB2BGR)\n",
    "                \n",
    "                temporary_list = []\n",
    "                temporary_list.append(results.pose_landmarks.landmark[mp.solutions.holistic.PoseLandmark.NOSE])\n",
    "                temporary_list.append(results.pose_landmarks.landmark[mp.solutions.holistic.PoseLandmark.LEFT_EYE])\n",
    "                temporary_list.append(results.pose_landmarks.landmark[mp.solutions.holistic.PoseLandmark.RIGHT_EYE])\n",
    "                temporary_list.append(results.pose_landmarks.landmark[mp.solutions.holistic.PoseLandmark.RIGHT_WRIST])\n",
    "                temporary_list.append(results.pose_landmarks.landmark[mp.solutions.holistic.PoseLandmark.LEFT_WRIST])\n",
    "                \n",
    "                detected.append(temporary_list)\n",
    "                \n",
    "                \n",
    "            else:\n",
    "                frames.pop(0)\n",
    "                detected.pop(0)\n",
    "                frames.append(cv2.resize(frame, (small_width, small_height)))\n",
    "\n",
    "                frame_detected = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                frame_detected.flags.writeable = False\n",
    "                results = holistic.process(frame_detected)\n",
    "                frame_detected.flags.writeable = True\n",
    "                frame_detected = cv2.cvtColor(frame_detected, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "                temporary_list = []\n",
    "                temporary_list.append(results.pose_landmarks.landmark[mp.solutions.holistic.PoseLandmark.NOSE])\n",
    "                temporary_list.append(results.pose_landmarks.landmark[mp.solutions.holistic.PoseLandmark.LEFT_EYE])\n",
    "                temporary_list.append(results.pose_landmarks.landmark[mp.solutions.holistic.PoseLandmark.RIGHT_EYE])\n",
    "                temporary_list.append(results.pose_landmarks.landmark[mp.solutions.holistic.PoseLandmark.RIGHT_WRIST])\n",
    "                temporary_list.append(results.pose_landmarks.landmark[mp.solutions.holistic.PoseLandmark.LEFT_WRIST])\n",
    "\n",
    "                detected.append(temporary_list)\n",
    "\n",
    "            if len(frames) >= 15:\n",
    "                stacked1_frames = np.vstack((frames[0], frames[1], frames[2]))\n",
    "                stacked2_frames = np.vstack((frames[3], frames[4], frames[5]))\n",
    "                stacked3_frames = np.vstack((frames[6], frames[7], frames[8]))\n",
    "                stacked4_frames = np.vstack((frames[9], frames[10], frames[11]))\n",
    "                stacked5_frames = np.vstack((frames[12], frames[13], frames[14]))\n",
    "\n",
    "                stacked1_frames = cv2.resize(stacked1_frames, (small_width, original_height))\n",
    "                stacked2_frames = cv2.resize(stacked2_frames, (small_width, original_height))\n",
    "                stacked3_frames = cv2.resize(stacked3_frames, (small_width, original_height))\n",
    "                stacked4_frames = cv2.resize(stacked4_frames, (small_width, original_height))\n",
    "                stacked5_frames = cv2.resize(stacked5_frames, (small_width, original_height))\n",
    "                \n",
    "                final_layout = np.hstack((frame, stacked1_frames, stacked2_frames, stacked3_frames, stacked4_frames, stacked5_frames))\n",
    "            else:\n",
    "                final_layout = frame\n",
    "\n",
    "            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            image.flags.writeable = False\n",
    "\n",
    "            # Flatten the list of lists in detected to a single list\n",
    "            flat_detected = [landmark for sublist in detected for landmark in sublist]\n",
    "            print(len(flat_detected))\n",
    "\n",
    "            landmark_list = landmark_pb2.NormalizedLandmarkList()\n",
    "            landmark_list.landmark.extend(flat_detected)\n",
    "\n",
    "            cv2.imshow(\"Camera Capture\", final_layout)\n",
    "            if cv2.waitKey (1) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "camera.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc748136",
   "metadata": {},
   "source": [
    "- Face Seen (face_s) = Forward & Backward\n",
    "- Face Direction (face_d) = Left & Right\n",
    "- Hand Shown (hand_s) = Shown & Hidden "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae06f435",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['class',\n",
       " 'x1',\n",
       " 'y1',\n",
       " 'z1',\n",
       " 'v1',\n",
       " 'x2',\n",
       " 'y2',\n",
       " 'z2',\n",
       " 'v2',\n",
       " 'x3',\n",
       " 'y3',\n",
       " 'z3',\n",
       " 'v3',\n",
       " 'x4',\n",
       " 'y4',\n",
       " 'z4',\n",
       " 'v4',\n",
       " 'x5',\n",
       " 'y5',\n",
       " 'z5',\n",
       " 'v5',\n",
       " 'face_d1',\n",
       " 'face_s1',\n",
       " 'hand_s1',\n",
       " 'x6',\n",
       " 'y6',\n",
       " 'z6',\n",
       " 'v6',\n",
       " 'x7',\n",
       " 'y7',\n",
       " 'z7',\n",
       " 'v7',\n",
       " 'x8',\n",
       " 'y8',\n",
       " 'z8',\n",
       " 'v8',\n",
       " 'x9',\n",
       " 'y9',\n",
       " 'z9',\n",
       " 'v9',\n",
       " 'x10',\n",
       " 'y10',\n",
       " 'z10',\n",
       " 'v10',\n",
       " 'face_d2',\n",
       " 'face_s2',\n",
       " 'hand_s2',\n",
       " 'x11',\n",
       " 'y11',\n",
       " 'z11',\n",
       " 'v11',\n",
       " 'x12',\n",
       " 'y12',\n",
       " 'z12',\n",
       " 'v12',\n",
       " 'x13',\n",
       " 'y13',\n",
       " 'z13',\n",
       " 'v13',\n",
       " 'x14',\n",
       " 'y14',\n",
       " 'z14',\n",
       " 'v14',\n",
       " 'x15',\n",
       " 'y15',\n",
       " 'z15',\n",
       " 'v15',\n",
       " 'face_d3',\n",
       " 'face_s3',\n",
       " 'hand_s3',\n",
       " 'x16',\n",
       " 'y16',\n",
       " 'z16',\n",
       " 'v16',\n",
       " 'x17',\n",
       " 'y17',\n",
       " 'z17',\n",
       " 'v17',\n",
       " 'x18',\n",
       " 'y18',\n",
       " 'z18',\n",
       " 'v18',\n",
       " 'x19',\n",
       " 'y19',\n",
       " 'z19',\n",
       " 'v19',\n",
       " 'x20',\n",
       " 'y20',\n",
       " 'z20',\n",
       " 'v20',\n",
       " 'face_d4',\n",
       " 'face_s4',\n",
       " 'hand_s4',\n",
       " 'x21',\n",
       " 'y21',\n",
       " 'z21',\n",
       " 'v21',\n",
       " 'x22',\n",
       " 'y22',\n",
       " 'z22',\n",
       " 'v22',\n",
       " 'x23',\n",
       " 'y23',\n",
       " 'z23',\n",
       " 'v23',\n",
       " 'x24',\n",
       " 'y24',\n",
       " 'z24',\n",
       " 'v24',\n",
       " 'x25',\n",
       " 'y25',\n",
       " 'z25',\n",
       " 'v25',\n",
       " 'face_d5',\n",
       " 'face_s5',\n",
       " 'hand_s5',\n",
       " 'x26',\n",
       " 'y26',\n",
       " 'z26',\n",
       " 'v26',\n",
       " 'x27',\n",
       " 'y27',\n",
       " 'z27',\n",
       " 'v27',\n",
       " 'x28',\n",
       " 'y28',\n",
       " 'z28',\n",
       " 'v28',\n",
       " 'x29',\n",
       " 'y29',\n",
       " 'z29',\n",
       " 'v29',\n",
       " 'x30',\n",
       " 'y30',\n",
       " 'z30',\n",
       " 'v30',\n",
       " 'face_d6',\n",
       " 'face_s6',\n",
       " 'hand_s6',\n",
       " 'x31',\n",
       " 'y31',\n",
       " 'z31',\n",
       " 'v31',\n",
       " 'x32',\n",
       " 'y32',\n",
       " 'z32',\n",
       " 'v32',\n",
       " 'x33',\n",
       " 'y33',\n",
       " 'z33',\n",
       " 'v33',\n",
       " 'x34',\n",
       " 'y34',\n",
       " 'z34',\n",
       " 'v34',\n",
       " 'x35',\n",
       " 'y35',\n",
       " 'z35',\n",
       " 'v35',\n",
       " 'face_d7',\n",
       " 'face_s7',\n",
       " 'hand_s7',\n",
       " 'x36',\n",
       " 'y36',\n",
       " 'z36',\n",
       " 'v36',\n",
       " 'x37',\n",
       " 'y37',\n",
       " 'z37',\n",
       " 'v37',\n",
       " 'x38',\n",
       " 'y38',\n",
       " 'z38',\n",
       " 'v38',\n",
       " 'x39',\n",
       " 'y39',\n",
       " 'z39',\n",
       " 'v39',\n",
       " 'x40',\n",
       " 'y40',\n",
       " 'z40',\n",
       " 'v40',\n",
       " 'face_d8',\n",
       " 'face_s8',\n",
       " 'hand_s8',\n",
       " 'x41',\n",
       " 'y41',\n",
       " 'z41',\n",
       " 'v41',\n",
       " 'x42',\n",
       " 'y42',\n",
       " 'z42',\n",
       " 'v42',\n",
       " 'x43',\n",
       " 'y43',\n",
       " 'z43',\n",
       " 'v43',\n",
       " 'x44',\n",
       " 'y44',\n",
       " 'z44',\n",
       " 'v44',\n",
       " 'x45',\n",
       " 'y45',\n",
       " 'z45',\n",
       " 'v45',\n",
       " 'face_d9',\n",
       " 'face_s9',\n",
       " 'hand_s9',\n",
       " 'x46',\n",
       " 'y46',\n",
       " 'z46',\n",
       " 'v46',\n",
       " 'x47',\n",
       " 'y47',\n",
       " 'z47',\n",
       " 'v47',\n",
       " 'x48',\n",
       " 'y48',\n",
       " 'z48',\n",
       " 'v48',\n",
       " 'x49',\n",
       " 'y49',\n",
       " 'z49',\n",
       " 'v49',\n",
       " 'x50',\n",
       " 'y50',\n",
       " 'z50',\n",
       " 'v50',\n",
       " 'face_d10',\n",
       " 'face_s10',\n",
       " 'hand_s10',\n",
       " 'x51',\n",
       " 'y51',\n",
       " 'z51',\n",
       " 'v51',\n",
       " 'x52',\n",
       " 'y52',\n",
       " 'z52',\n",
       " 'v52',\n",
       " 'x53',\n",
       " 'y53',\n",
       " 'z53',\n",
       " 'v53',\n",
       " 'x54',\n",
       " 'y54',\n",
       " 'z54',\n",
       " 'v54',\n",
       " 'x55',\n",
       " 'y55',\n",
       " 'z55',\n",
       " 'v55',\n",
       " 'face_d11',\n",
       " 'face_s11',\n",
       " 'hand_s11',\n",
       " 'x56',\n",
       " 'y56',\n",
       " 'z56',\n",
       " 'v56',\n",
       " 'x57',\n",
       " 'y57',\n",
       " 'z57',\n",
       " 'v57',\n",
       " 'x58',\n",
       " 'y58',\n",
       " 'z58',\n",
       " 'v58',\n",
       " 'x59',\n",
       " 'y59',\n",
       " 'z59',\n",
       " 'v59',\n",
       " 'x60',\n",
       " 'y60',\n",
       " 'z60',\n",
       " 'v60',\n",
       " 'face_d12',\n",
       " 'face_s12',\n",
       " 'hand_s12',\n",
       " 'x61',\n",
       " 'y61',\n",
       " 'z61',\n",
       " 'v61',\n",
       " 'x62',\n",
       " 'y62',\n",
       " 'z62',\n",
       " 'v62',\n",
       " 'x63',\n",
       " 'y63',\n",
       " 'z63',\n",
       " 'v63',\n",
       " 'x64',\n",
       " 'y64',\n",
       " 'z64',\n",
       " 'v64',\n",
       " 'x65',\n",
       " 'y65',\n",
       " 'z65',\n",
       " 'v65',\n",
       " 'face_d13',\n",
       " 'face_s13',\n",
       " 'hand_s13',\n",
       " 'x66',\n",
       " 'y66',\n",
       " 'z66',\n",
       " 'v66',\n",
       " 'x67',\n",
       " 'y67',\n",
       " 'z67',\n",
       " 'v67',\n",
       " 'x68',\n",
       " 'y68',\n",
       " 'z68',\n",
       " 'v68',\n",
       " 'x69',\n",
       " 'y69',\n",
       " 'z69',\n",
       " 'v69',\n",
       " 'x70',\n",
       " 'y70',\n",
       " 'z70',\n",
       " 'v70',\n",
       " 'face_d14',\n",
       " 'face_s14',\n",
       " 'hand_s14',\n",
       " 'x71',\n",
       " 'y71',\n",
       " 'z71',\n",
       " 'v71',\n",
       " 'x72',\n",
       " 'y72',\n",
       " 'z72',\n",
       " 'v72',\n",
       " 'x73',\n",
       " 'y73',\n",
       " 'z73',\n",
       " 'v73',\n",
       " 'x74',\n",
       " 'y74',\n",
       " 'z74',\n",
       " 'v74',\n",
       " 'x75',\n",
       " 'y75',\n",
       " 'z75',\n",
       " 'v75',\n",
       " 'face_d15',\n",
       " 'face_s15',\n",
       " 'hand_s15']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "used_coords = len(landmark_list.landmark)\n",
    "used_coords\n",
    "frames = 1\n",
    "\n",
    "landmarks = ['class']\n",
    "for val in range(1, used_coords+1):\n",
    "    landmarks += ['x{}'.format(val), 'y{}'.format(val), 'z{}'.format(val), 'v{}'.format(val)]\n",
    "    if val % 5 == 0:\n",
    "        landmarks += ['face_d{}'.format(frames), 'face_s{}'.format(frames), 'hand_s{}'.format(frames)]\n",
    "        frames += 1\n",
    "\n",
    "\n",
    "landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54ae9b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_csv = 'dataset/csv/ds_extended_v1.csv'\n",
    "os.makedirs(os.path.dirname(file_csv), exist_ok=True)\n",
    "\n",
    "with open(file_csv, mode='w', newline='') as f:\n",
    "    csv_writer = csv.writer(f, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "    csv_writer.writerow(landmarks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e35f19",
   "metadata": {},
   "source": [
    "# Capture Dataset With Camera (Experimental)\n",
    "the output is to creating and collecting dataset that will be trained\n",
    "\n",
    "\n",
    "planning :\n",
    "- create a countdown, while the user do something\n",
    "- after a countdown stop, capture the last 15 frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16137bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def side_state(state, lndmrkX):\n",
    "    if state == 0:\n",
    "        if lndmrkX[\"nose\"] == min(lndmrkX.values()):\n",
    "            direction = 1 # Kiri\n",
    "        elif lndmrkX[\"nose\"] == max(lndmrkX.values()):      \n",
    "            direction = 2 # Kanan\n",
    "        else:\n",
    "            direction = 0 # Tengah\n",
    "    else:\n",
    "        if lndmrkX[\"nose\"] == min(lndmrkX.values()):\n",
    "            direction = 2 # Kanan\n",
    "        elif lndmrkX[\"nose\"] == max(lndmrkX.values()):      \n",
    "            direction = 1 # Kiri\n",
    "        else:\n",
    "            direction = 0 # Tengah\n",
    "    \n",
    "    return direction\n",
    "\n",
    "def hand_state(state, lndmrkZ):\n",
    "    if state == 0:\n",
    "        if lndmrkZ[\"wrist_r\"] and lndmrkZ[\"wrist_l\"] < lndmrkZ[\"nose\"]:\n",
    "            hand = 1 # Terlihat\n",
    "        else:\n",
    "            hand = 0 # Tidak Terlihat\n",
    "    else:\n",
    "        if lndmrkZ[\"wrist_r\"] and lndmrkZ[\"wrist_l\"] < lndmrkZ[\"nose\"]:\n",
    "            hand = 0 # Terlihat\n",
    "        else:\n",
    "            hand = 1 # Tidak Terlihat\n",
    "    return hand\n",
    "\n",
    "def get_extFeature_value(lndmrk):\n",
    "    noseX, noseY, noseZ = lndmrk[\"nose\"].x, lndmrk[\"nose\"].y, lndmrk[\"nose\"].z\n",
    "    earLX, earLY, earLZ = lndmrk[\"ear_l\"].x, lndmrk[\"ear_l\"].y, lndmrk[\"ear_l\"].z\n",
    "    earRX, earRY, earRZ = lndmrk[\"ear_r\"].x, lndmrk[\"ear_r\"].y, lndmrk[\"ear_r\"].z\n",
    "    wristRX, wristRY, wristRZ = lndmrk[\"wrist_r\"].x, lndmrk[\"wrist_r\"].y, lndmrk[\"wrist_r\"].z\n",
    "    wristLX, wristLY, wristLZ = lndmrk[\"wrist_l\"].x, lndmrk[\"wrist_l\"].y, lndmrk[\"wrist_l\"].z\n",
    "    \n",
    "    lndmrkX = {\"nose\": noseX, \"ear_l\": earLX, \"ear_r\": earRX, \"wrist_r\": wristRX, \"wrist_l\": wristLX }\n",
    "    lndmrkY = {\"nose\": noseY, \"ear_l\": earLY, \"ear_r\": earRY, \"wrist_r\": wristRY, \"wrist_l\": wristLY }\n",
    "    lndmrkZ = {\"nose\": noseZ, \"ear_l\": earLZ, \"ear_r\": earRZ, \"wrist_r\": wristRZ, \"wrist_l\": wristLZ }\n",
    "\n",
    "    if noseZ < (earLZ and earRZ):\n",
    "        state = 0\n",
    "        side = side_state(0, lndmrkX)\n",
    "        hand = hand_state(0, lndmrkZ)\n",
    "    else:\n",
    "        state = 1\n",
    "        side = side_state(1, lndmrkX)\n",
    "        hand = hand_state(1, lndmrkZ)\n",
    "\n",
    "    return side, state, hand\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb20e62",
   "metadata": {},
   "source": [
    "# - NOTE -\n",
    "1. West, East detection where we see as additional of parameter when detecting suspicious activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "633929fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_dataset(gerakan, repeat):\n",
    "    for i in range(0, repeat):\n",
    "        class_name = gerakan\n",
    "        camera = cv2.VideoCapture(ip_cam)\n",
    "\n",
    "        frames = []\n",
    "        detected = []\n",
    "\n",
    "        # ==== EXTENDED FEATURE ====\n",
    "        face_direction = []\n",
    "        face_shown = []\n",
    "        hand_shown = []\n",
    "\n",
    "        desire_fps = 5\n",
    "        frame_delay = 1.0 / desire_fps\n",
    "\n",
    "        if not camera.isOpened():\n",
    "            print(\"Error: Could not open camera.\")\n",
    "        else:\n",
    "            print(\"Camera is ready. Press 'q' to quit.\")\n",
    "\n",
    "        with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "            start_time = time.time()\n",
    "            \n",
    "            while camera.isOpened:\n",
    "                delay_frame_time = time.time()\n",
    "                countdown_time = 10\n",
    "                \n",
    "                ret, frame = camera.read()\n",
    "                \n",
    "                if ret:\n",
    "\n",
    "                    frame = cv2.resize(frame, (original_width, original_height))\n",
    "\n",
    "                    if len(frames) < num_frames:\n",
    "                        frames.append(cv2.resize(frame, (small_width, small_height)))\n",
    "                        \n",
    "                        frame_detected = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                        frame_detected.flags.writeable = False\n",
    "                        results = holistic.process(frame_detected)\n",
    "                        frame_detected.flags.writeable = True\n",
    "                        frame_detected = cv2.cvtColor(frame_detected, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "                        nose = results.pose_landmarks.landmark[mp.solutions.holistic.PoseLandmark.NOSE]\n",
    "                        ear_r = results.pose_landmarks.landmark[mp.solutions.holistic.PoseLandmark.RIGHT_EAR]\n",
    "                        ear_l = results.pose_landmarks.landmark[mp.solutions.holistic.PoseLandmark.LEFT_EAR]\n",
    "                        wrist_r = results.pose_landmarks.landmark[mp.solutions.holistic.PoseLandmark.RIGHT_WRIST]\n",
    "                        wrist_l = results.pose_landmarks.landmark[mp.solutions.holistic.PoseLandmark.RIGHT_WRIST]\n",
    "\n",
    "                        temporary_list = []\n",
    "                        temporary_list.append(nose)\n",
    "                        temporary_list.append(ear_l)\n",
    "                        temporary_list.append(ear_r)\n",
    "                        temporary_list.append(wrist_r)\n",
    "                        temporary_list.append(wrist_l)\n",
    "                        \n",
    "                        extended_feature = get_extFeature_value({\"nose\": nose, \"ear_r\": ear_r, \"ear_l\": ear_l, \"wrist_r\": wrist_r, \"wrist_l\": wrist_l})\n",
    "                        face_direction.append(extended_feature[0])\n",
    "                        face_shown.append(extended_feature[1])\n",
    "                        hand_shown.append(extended_feature[2])\n",
    "\n",
    "                        detected.append(temporary_list)\n",
    "                        \n",
    "                    else:\n",
    "                        frames.pop(0)\n",
    "\n",
    "                        face_direction.pop(0)\n",
    "                        face_shown.pop(0)\n",
    "                        hand_shown.pop(0)\n",
    "\n",
    "                        detected.pop(0)\n",
    "                        frames.append(cv2.resize(frame, (small_width, small_height)))\n",
    "\n",
    "                        frame_detected = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                        frame_detected.flags.writeable = False\n",
    "                        results = holistic.process(frame_detected)\n",
    "                        frame_detected.flags.writeable = True\n",
    "                        frame_detected = cv2.cvtColor(frame_detected, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "                        nose = results.pose_landmarks.landmark[mp.solutions.holistic.PoseLandmark.NOSE]\n",
    "                        ear_r = results.pose_landmarks.landmark[mp.solutions.holistic.PoseLandmark.RIGHT_EAR]\n",
    "                        ear_l = results.pose_landmarks.landmark[mp.solutions.holistic.PoseLandmark.LEFT_EAR]\n",
    "                        wrist_r = results.pose_landmarks.landmark[mp.solutions.holistic.PoseLandmark.RIGHT_WRIST]\n",
    "                        wrist_l = results.pose_landmarks.landmark[mp.solutions.holistic.PoseLandmark.RIGHT_WRIST]\n",
    "                        \n",
    "                        temporary_list = []\n",
    "                        temporary_list.append(nose)\n",
    "                        temporary_list.append(ear_l)\n",
    "                        temporary_list.append(ear_r)\n",
    "                        temporary_list.append(wrist_r)\n",
    "                        temporary_list.append(wrist_l)\n",
    "\n",
    "                        extended_feature = get_extFeature_value({\"nose\": nose, \"ear_r\": ear_r, \"ear_l\": ear_l, \"wrist_r\": wrist_r, \"wrist_l\": wrist_l})\n",
    "                        face_direction.append(extended_feature[0])\n",
    "                        face_shown.append(extended_feature[1])\n",
    "                        hand_shown.append(extended_feature[2])\n",
    "\n",
    "                        detected.append(temporary_list)\n",
    "\n",
    "                    if len(frames) >= 15:\n",
    "                        stacked1_frames = np.vstack((frames[0], frames[1], frames[2]))\n",
    "                        stacked2_frames = np.vstack((frames[3], frames[4], frames[5]))\n",
    "                        stacked3_frames = np.vstack((frames[6], frames[7], frames[8]))\n",
    "                        stacked4_frames = np.vstack((frames[9], frames[10], frames[11]))\n",
    "                        stacked5_frames = np.vstack((frames[12], frames[13], frames[14]))\n",
    "\n",
    "                        stacked1_frames = cv2.resize(stacked1_frames, (small_width, original_height))\n",
    "                        stacked2_frames = cv2.resize(stacked2_frames, (small_width, original_height))\n",
    "                        stacked3_frames = cv2.resize(stacked3_frames, (small_width, original_height))\n",
    "                        stacked4_frames = cv2.resize(stacked4_frames, (small_width, original_height))\n",
    "                        stacked5_frames = cv2.resize(stacked5_frames, (small_width, original_height))\n",
    "                        \n",
    "                        final_layout = np.hstack((frame, stacked1_frames, stacked2_frames, stacked3_frames, stacked4_frames, stacked5_frames))\n",
    "                    else:\n",
    "                        final_layout = frame\n",
    "\n",
    "                    image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                    image.flags.writeable = False\n",
    "\n",
    "                    flat_detected = [landmark for sublist in detected for landmark in sublist]\n",
    "\n",
    "                    landmark_list = landmark_pb2.NormalizedLandmarkList()\n",
    "                    landmark_list.landmark.extend(flat_detected)\n",
    "                    \n",
    "                    cv2.imshow(\"Camera Capture\", final_layout)\n",
    "\n",
    "                    # Print the elapsed time in seconds since the start of the loop\n",
    "                    \n",
    "                    fps = time.time() - delay_frame_time\n",
    "                    sleep_time = max(0, frame_delay - fps)\n",
    "                    \n",
    "                    time.sleep(sleep_time)\n",
    "\n",
    "                    elapsed = time.time() - start_time\n",
    "                    if elapsed <= 10:\n",
    "                        print(f\"Elapsed time: {elapsed} seconds\")\n",
    "                            \n",
    "                    else:\n",
    "                        lv = landmark_list.landmark\n",
    "\n",
    "                        counter = 0\n",
    "                        featureper_frames = 0\n",
    "                        dataset_row = []\n",
    "                        \n",
    "                        for lndmrk in lv:\n",
    "                            dataset_row.append(lndmrk.x)\n",
    "                            dataset_row.append(lndmrk.y)\n",
    "                            dataset_row.append(lndmrk.z)\n",
    "                            dataset_row.append(lndmrk.visibility)\n",
    "                            counter += 1\n",
    "\n",
    "                            if counter % 5 == 0:\n",
    "                                dataset_row.append(face_direction[featureper_frames])\n",
    "                                dataset_row.append(face_shown[featureper_frames])\n",
    "                                dataset_row.append(hand_shown[featureper_frames])\n",
    "                                featureper_frames += 1 \n",
    "\n",
    "                        dataset_row = list(np.array(dataset_row))\n",
    "                        dataset_row.insert(0, class_name)\n",
    "\n",
    "                        with open(file_csv, mode='a', newline='') as f:\n",
    "                            csv_writer = csv.writer(f, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "                            csv_writer.writerow(dataset_row)\n",
    "                        break\n",
    "\n",
    "                    if cv2.waitKey (1) & 0xFF == ord('q'):\n",
    "                        break\n",
    "\n",
    "        camera.release()\n",
    "        cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bc473f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Camera is ready. Press 'q' to quit.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1756978961.387447   42331 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1756978961.423196   48164 gl_context.cc:369] GL version: 3.1 (OpenGL ES 3.1 Mesa 24.0.9-0ubuntu0.3), renderer: D3D12 (AMD Radeon(TM) Graphics)\n",
      "W0000 00:00:1756978961.755564   48151 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1756978961.958208   48152 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1756978961.974005   48155 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1756978961.975210   48157 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1756978961.979772   48162 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1756978962.003965   48158 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1756978962.014204   48156 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1756978962.036417   48153 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 0.20516681671142578 seconds\n",
      "Elapsed time: 0.43171143531799316 seconds\n",
      "Elapsed time: 0.6355397701263428 seconds\n",
      "Elapsed time: 0.8405375480651855 seconds\n",
      "Elapsed time: 1.048640251159668 seconds\n",
      "Elapsed time: 1.2561795711517334 seconds\n",
      "Elapsed time: 1.4605071544647217 seconds\n",
      "Elapsed time: 1.668332576751709 seconds\n",
      "Elapsed time: 1.8759698867797852 seconds\n",
      "Elapsed time: 2.0841479301452637 seconds\n",
      "Elapsed time: 2.2884609699249268 seconds\n",
      "Elapsed time: 2.4969096183776855 seconds\n",
      "Elapsed time: 2.7020232677459717 seconds\n",
      "Elapsed time: 2.9072437286376953 seconds\n",
      "Elapsed time: 3.112478017807007 seconds\n",
      "Elapsed time: 3.3395462036132812 seconds\n",
      "Elapsed time: 3.546444892883301 seconds\n",
      "Elapsed time: 3.7530038356781006 seconds\n",
      "Elapsed time: 3.9593796730041504 seconds\n",
      "Elapsed time: 4.165727615356445 seconds\n",
      "Elapsed time: 4.369850158691406 seconds\n",
      "Elapsed time: 4.574939012527466 seconds\n",
      "Elapsed time: 4.780186653137207 seconds\n",
      "Elapsed time: 4.98513650894165 seconds\n",
      "Elapsed time: 5.189488887786865 seconds\n",
      "Elapsed time: 5.393386363983154 seconds\n",
      "Elapsed time: 5.600815057754517 seconds\n",
      "Elapsed time: 5.804791450500488 seconds\n",
      "Elapsed time: 6.012464761734009 seconds\n",
      "Elapsed time: 6.22069787979126 seconds\n",
      "Elapsed time: 6.424700498580933 seconds\n",
      "Elapsed time: 6.633566856384277 seconds\n",
      "Elapsed time: 6.8410468101501465 seconds\n",
      "Elapsed time: 7.045731544494629 seconds\n",
      "Elapsed time: 7.254258394241333 seconds\n",
      "Elapsed time: 7.459853410720825 seconds\n",
      "Elapsed time: 7.6687610149383545 seconds\n",
      "Elapsed time: 7.877310514450073 seconds\n",
      "Elapsed time: 8.085376024246216 seconds\n",
      "Elapsed time: 8.289744853973389 seconds\n",
      "Elapsed time: 8.496192693710327 seconds\n",
      "Elapsed time: 8.700131893157959 seconds\n",
      "Elapsed time: 8.908713102340698 seconds\n",
      "Elapsed time: 9.116731643676758 seconds\n",
      "Elapsed time: 9.32080364227295 seconds\n",
      "Elapsed time: 9.524693727493286 seconds\n",
      "Elapsed time: 9.72945523262024 seconds\n",
      "Elapsed time: 9.93726897239685 seconds\n"
     ]
    }
   ],
   "source": [
    "extract_dataset(\"CelingakCelinguk\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440347b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "motion_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
