{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe59f71c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/c/users/nawfal/documents/apps/collaborative project/cctv_motion_detection/motion_venv/bin/python'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cdac84c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b13a49",
   "metadata": {},
   "source": [
    "# Camera Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f5d578e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cap = cv2.VideoCapture(\"http://192.168.50.234:5000/video\")\n",
    "cap = cv2.VideoCapture(\"http://192.168.100.197:5000/video\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1aa3111a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Camera is ready. Press 'q' to quit.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the camera\n",
    "\n",
    "# Check if the camera opened successfully\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open camera.\")\n",
    "else:\n",
    "    print(\"Camera is ready. Press 'q' to quit.\")\n",
    "\n",
    "# Capture frames from the camera\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to grab frame.\")\n",
    "        break\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.namedWindow(\"Camera Capture\", cv2.WINDOW_NORMAL | cv2.WINDOW_GUI_NORMAL)\n",
    "    cv2.resizeWindow(\"Camera Capture\", 640, 480)\n",
    "    cv2.imshow(\"Camera Capture\", frame)\n",
    "\n",
    "    # Exit the loop when 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the camera and close all OpenCV windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9697c8",
   "metadata": {},
   "source": [
    "# Import Mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a7ade97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-02 09:23:09.689425: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1756776189.936104     860 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1756776190.008722     860 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1756776190.615626     860 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1756776190.615658     860 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1756776190.615660     860 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1756776190.615662     860 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-09-02 09:23:10.697995: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import time\n",
    "import csv\n",
    "import warnings\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import mediapipe as mp\n",
    "from mediapipe.framework.formats import landmark_pb2\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils # Drawing helpers\n",
    "mp_holistic = mp.solutions.holistic # Mediapipe Solutions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230d207b",
   "metadata": {},
   "source": [
    "# Z Coordinate detection\n",
    "Zoom in & Zoom Out while move forward and backwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "112c0e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_nose_z(lndmrk_z):\n",
    "    nose = lndmrk_z[\"nose\"]\n",
    "    ear_l = lndmrk_z[\"ear_l\"]\n",
    "    ear_r = lndmrk_z[\"ear_r\"]\n",
    "\n",
    "    if nose < (ear_l and ear_r):\n",
    "        return \"Hadap Depan\"\n",
    "    else:\n",
    "        return \"Hadap Belakang\"\n",
    "\n",
    "    return f\"nose: {nose:.5f}, ear: {ear_l:.5f}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ebdeb05e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1756793829.961641     860 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1756793829.991000   54112 gl_context.cc:369] GL version: 3.1 (OpenGL ES 3.1 Mesa 24.0.9-0ubuntu0.3), renderer: D3D12 (AMD Radeon(TM) Graphics)\n",
      "W0000 00:00:1756793830.388888   54100 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1756793830.627939   54101 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1756793830.671005   54105 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1756793830.714023   54109 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1756793830.717157   54107 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1756793830.764209   54108 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1756793830.815160   54106 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1756793831.128511   54109 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "lndmrkX = None\n",
    "lndmrkY = None\n",
    "lndmrkZ = None\n",
    "\n",
    "cap = cv2.VideoCapture(\"http://192.168.100.197:5000/video\")\n",
    "\n",
    "\n",
    "time.sleep(3)\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic :\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Recolor Feed\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False        \n",
    "        \n",
    "        # Make Detections\n",
    "        results = holistic.process(image)\n",
    "\n",
    "        # Recolor image back to BGR for rendering\n",
    "        image.flags.writeable = True   \n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        # Get specific landmarks\n",
    "        nose = results.pose_landmarks.landmark[mp.solutions.holistic.PoseLandmark.NOSE]\n",
    "        ear_l = results.pose_landmarks.landmark[mp.solutions.holistic.PoseLandmark.LEFT_EAR]\n",
    "        ear_r = results.pose_landmarks.landmark[mp.solutions.holistic.PoseLandmark.RIGHT_EAR]\n",
    "\n",
    "        show_landmark_list = landmark_pb2.NormalizedLandmarkList()\n",
    "        show_landmark_list.landmark.extend([nose])\n",
    "\n",
    "        nose_z = check_nose_z({\"nose\": nose.z, \"ear_l\": ear_l.z, \"ear_r\": ear_r.z})\n",
    "\n",
    "        # Draw landmarks\n",
    "        for lndmrk_z in show_landmark_list.landmark:\n",
    "            x, y = int(lndmrk_z.x * image.shape[1]), int(lndmrk_z.y * image.shape[0])\n",
    "            lndmrkX = lndmrk_z.x\n",
    "            lndmrkY = lndmrk_z.y\n",
    "            lndmrkZ = lndmrk_z.z\n",
    "\n",
    "        cv2.putText(\n",
    "            image,\n",
    "            f\"x: ({nose_z})\",\n",
    "            (50, 50),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            0.7,\n",
    "            (0, 255, 0),\n",
    "            2,\n",
    "            cv2.LINE_AA\n",
    "        )\n",
    "        cv2.imshow('Raw Webcam Feed', image)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3385f7e5",
   "metadata": {},
   "source": [
    "move forward goes till minus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fdce0eb",
   "metadata": {},
   "source": [
    "# Face Direction Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc29e05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_direction(landmark):\n",
    "    nose = landmark[\"nose\"]\n",
    "    ear_r = landmark[\"ear_r\"]\n",
    "    ear_l = landmark[\"ear_l\"]\n",
    "\n",
    "    # Calculate distances\n",
    "    dist_left = abs(nose - ear_l)\n",
    "    dist_right = abs(nose - ear_r)\n",
    "    ear_gap = abs(ear_r - ear_l)\n",
    "\n",
    "    # Thresholds (tune based on your camera and position)\n",
    "    center_threshold = 0.05    # nose roughly centered\n",
    "    side_threshold = 0.02      # nose very close to one ear (90-degree)\n",
    "    min_ear_gap = 0.05         # ears almost overlapped in X axis at side profile\n",
    "\n",
    "    # Detect 90-degree turns first\n",
    "    if ear_gap < min_ear_gap:\n",
    "        if dist_right < dist_left:\n",
    "            return \"90-right\"\n",
    "        else:\n",
    "            return \"90-left\"\n",
    "\n",
    "    # Detect regular left/right/center\n",
    "    if dist_left - dist_right > center_threshold:\n",
    "        return \"left\"\n",
    "    elif dist_right - dist_left > center_threshold:\n",
    "        return \"right\"\n",
    "    else:\n",
    "        return \"center\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96ee06f",
   "metadata": {},
   "source": [
    "# Camera Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c5aed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Camera is ready. Press 'q' to quit.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1756785321.547105     860 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1756785321.597987   28259 gl_context.cc:369] GL version: 3.1 (OpenGL ES 3.1 Mesa 24.0.9-0ubuntu0.3), renderer: D3D12 (AMD Radeon(TM) Graphics)\n",
      "W0000 00:00:1756785322.221218   28249 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1756785322.316595   28257 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1756785322.331171   28248 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1756785322.331452   28249 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1756785322.376875   28248 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1756785322.381234   28246 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1756785322.393043   28255 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1756785322.396786   28253 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "camera = cv2.VideoCapture(\"http://192.168.100.197:5000/video\")\n",
    "\n",
    "# Check if the camera opened successfully\n",
    "if not camera.isOpened():\n",
    "    print(\"Error: Could not open camera.\")\n",
    "else:\n",
    "    print(\"Camera is ready. Press 'q' to quit.\")\n",
    "\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.1, min_tracking_confidence=0.1) as holistic :\n",
    "    \n",
    "    while camera.isOpened():  \n",
    "        ret, frame = camera.read()\n",
    "\n",
    "        if not ret:\n",
    "            print(\"❌ Failed to grab frame\")\n",
    "            break\n",
    "        \n",
    "        # Recolor Feed\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False        \n",
    "        \n",
    "        # Make Detections\n",
    "        results = holistic.process(image)\n",
    "\n",
    "        # Recolor image back to BGR for rendering\n",
    "        image.flags.writeable = True   \n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        # =============== THE LANDMARKS ============== #\n",
    "        \n",
    "        nose = results.pose_landmarks.landmark[mp.solutions.holistic.PoseLandmark.NOSE]\n",
    "        ear_r = results.pose_landmarks.landmark[mp.solutions.holistic.PoseLandmark.RIGHT_EAR]\n",
    "        ear_l = results.pose_landmarks.landmark[mp.solutions.holistic.PoseLandmark.LEFT_EAR]\n",
    "        wrist_r = results.pose_landmarks.landmark[mp.solutions.holistic.PoseLandmark.RIGHT_WRIST]\n",
    "\n",
    "        # Array Each Landmark\n",
    "        face_direction = check_direction({\"nose\": nose.x, \"ear_r\": ear_r.x, \"ear_l\": ear_l.x}) \n",
    "\n",
    "        # For Draw\n",
    "        lm = landmark_pb2.NormalizedLandmarkList()\n",
    "        lm.landmark.extend([nose, ear_r, ear_l, wrist_r])\n",
    "\n",
    "        # ============================================ #\n",
    "\n",
    "        for lndmrk_z in lm.landmark:\n",
    "            x, y = int(lndmrk_z.x * image.shape[1]), int(lndmrk_z.y * image.shape[0])\n",
    "            cv2.circle(image, (x, y), 5, (255, 0, 0), -1)\n",
    "        \n",
    "            cv2.putText(\n",
    "                image,\n",
    "                f\"x: ({face_direction})\",\n",
    "                (50, 50),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                0.7,\n",
    "                (0, 255, 0),\n",
    "                2,\n",
    "                cv2.LINE_AA\n",
    "            )\n",
    "        # =============== DETECTION ================== #\n",
    "        # ============================================ #\n",
    "\n",
    "        cv2.imshow('Raw Webcam Feed', image)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "camera.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e014f80b",
   "metadata": {},
   "source": [
    "Left & Right Eye Can Be Used For the Left and Right"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d28761",
   "metadata": {},
   "source": [
    "# Body Rotation State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d6fe24",
   "metadata": {},
   "outputs": [],
   "source": [
    "camera = cv2.VideoCapture(\"http://192.168.100.197:5000/video\")\n",
    "\n",
    "# Check if the camera opened successfully\n",
    "if not camera.isOpened():\n",
    "    print(\"Error: Could not open camera.\")\n",
    "else:\n",
    "    print(\"Camera is ready. Press 'q' to quit.\")\n",
    "\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.1, min_tracking_confidence=0.1) as holistic :\n",
    "    \n",
    "    state = \"\"\n",
    "\n",
    "    while camera.isOpened():  \n",
    "        ret, frame = camera.read()\n",
    "\n",
    "        if not ret:\n",
    "            print(\"❌ Failed to grab frame\")\n",
    "            break\n",
    "        \n",
    "        # Recolor Feed\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False        \n",
    "        \n",
    "        # Make Detections\n",
    "        results = holistic.process(image)\n",
    "\n",
    "        # Recolor image back to BGR for rendering\n",
    "        image.flags.writeable = True   \n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        # =============== THE LANDMARKS ============== #\n",
    "        \n",
    "        nose = results.pose_landmarks.landmark[mp.solutions.holistic.PoseLandmark.NOSE]\n",
    "        ear_r = results.pose_landmarks.landmark[mp.solutions.holistic.PoseLandmark.RIGHT_EAR]\n",
    "        ear_l = results.pose_landmarks.landmark[mp.solutions.holistic.PoseLandmark.LEFT_EAR]\n",
    "        wrist_r = results.pose_landmarks.landmark[mp.solutions.holistic.PoseLandmark.RIGHT_WRIST]\n",
    "\n",
    "        # Array Each Landmark\n",
    "        # face_direction = check_direction({\"nose\": nose.x, \"ear_r\": ear_r.x, \"ear_l\": ear_l.x}) \n",
    "\n",
    "        # For Draw\n",
    "        lm = landmark_pb2.NormalizedLandmarkList()\n",
    "        lm.landmark.extend([nose, ear_r, ear_l, wrist_r])\n",
    "\n",
    "        # ============================================ #\n",
    "\n",
    "        for lndmrk_z in lm.landmark:\n",
    "            x, y = int(lndmrk_z.x * image.shape[1]), int(lndmrk_z.y * image.shape[0])\n",
    "            cv2.circle(image, (x, y), 5, (255, 0, 0), -1)\n",
    "        \n",
    "            # cv2.putText(\n",
    "            #     image,\n",
    "            #     f\"x: ({face_direction})\",\n",
    "            #     (50, 50),\n",
    "            #     cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            #     0.7,\n",
    "            #     (0, 255, 0),\n",
    "            #     2,\n",
    "            #     cv2.LINE_AA\n",
    "            # )\n",
    "        # =============== DETECTION ================== #\n",
    "        # ============================================ #\n",
    "\n",
    "        cv2.imshow('Raw Webcam Feed', image)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "camera.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "motion_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
