{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe59f71c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/c/users/nawfal/documents/apps/collaborative project/cctv_motion_detection/motion_venv/bin/python'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cdac84c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b13a49",
   "metadata": {},
   "source": [
    "# Camera Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2f5d578e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cap = cv2.VideoCapture(\"http://192.168.50.234:5000/video\")\n",
    "cap = cv2.VideoCapture(\"../test/vidio/record_1.mp4\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1aa3111a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Camera is ready. Press 'q' to quit.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the camera\n",
    "\n",
    "# Check if the camera opened successfully\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open camera.\")\n",
    "else:\n",
    "    print(\"Camera is ready. Press 'q' to quit.\")\n",
    "\n",
    "# Capture frames from the camera\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to grab frame.\")\n",
    "        break\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.namedWindow(\"Camera Capture\", cv2.WINDOW_NORMAL | cv2.WINDOW_GUI_NORMAL)\n",
    "    cv2.resizeWindow(\"Camera Capture\", 640, 480)\n",
    "    cv2.imshow(\"Camera Capture\", frame)\n",
    "\n",
    "    # Exit the loop when 'q' is pressed\n",
    "    if cv2.waitKey(22) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the camera and close all OpenCV windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9697c8",
   "metadata": {},
   "source": [
    "# Import Mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a7ade97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-04 16:42:36.754038: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1756975356.988810     868 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1756975357.058363     868 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1756975357.666045     868 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1756975357.666087     868 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1756975357.666089     868 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1756975357.666091     868 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-09-04 16:42:37.747035: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import time\n",
    "import csv\n",
    "import warnings\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import mediapipe as mp\n",
    "from mediapipe.framework.formats import landmark_pb2\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils # Drawing helpers\n",
    "mp_holistic = mp.solutions.holistic # Mediapipe Solutions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230d207b",
   "metadata": {},
   "source": [
    "# Z Coordinate detection\n",
    "Zoom in & Zoom Out while move forward and backwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "112c0e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_nose_z(lndmrk_z):\n",
    "    nose = lndmrk_z[\"nose\"]\n",
    "    ear_l = lndmrk_z[\"ear_l\"]\n",
    "    ear_r = lndmrk_z[\"ear_r\"]\n",
    "\n",
    "    if nose < (ear_l and ear_r):\n",
    "        return \"Hadap Depan\"\n",
    "    else:\n",
    "        return \"Hadap Belakang\"\n",
    "\n",
    "    return f\"nose: {nose:.5f}, ear: {ear_l:.5f}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdeb05e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1756882411.452240  116717 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1756882411.486454  119074 gl_context.cc:369] GL version: 3.1 (OpenGL ES 3.1 Mesa 24.0.9-0ubuntu0.3), renderer: D3D12 (AMD Radeon(TM) Graphics)\n",
      "W0000 00:00:1756882411.899384  119064 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1756882412.072719  119066 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1756882412.086042  119064 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1756882412.087901  119071 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1756882412.096635  119065 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1756882412.114195  119072 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1756882412.141257  119069 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1756882412.153213  119068 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "lndmrkX = None\n",
    "lndmrkY = None\n",
    "lndmrkZ = None\n",
    "\n",
    "# cap = cv2.VideoCapture(\"http://192.168.100.197:5000/video\")\n",
    "cap = cv2.VideoCapture(\"../test/vidio/record_1.mp4\")\n",
    "\n",
    "\n",
    "time.sleep(3)\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic :\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Recolor Feed\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False        \n",
    "        \n",
    "        # Make Detections\n",
    "        results = holistic.process(image)\n",
    "\n",
    "        # Recolor image back to BGR for rendering\n",
    "        image.flags.writeable = True   \n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        try:\n",
    "            # Get specific landmarks\n",
    "            nose = results.pose_landmarks.landmark[mp.solutions.holistic.PoseLandmark.NOSE]\n",
    "            ear_l = results.pose_landmarks.landmark[mp.solutions.holistic.PoseLandmark.LEFT_EAR]\n",
    "            ear_r = results.pose_landmarks.landmark[mp.solutions.holistic.PoseLandmark.RIGHT_EAR]\n",
    "\n",
    "            show_landmark_list = landmark_pb2.NormalizedLandmarkList()\n",
    "            show_landmark_list.landmark.extend([nose])\n",
    "\n",
    "            nose_z = check_nose_z({\"nose\": nose.z, \"ear_l\": ear_l.z, \"ear_r\": ear_r.z})\n",
    "\n",
    "            # Draw landmarks\n",
    "            for lndmrk_z in show_landmark_list.landmark:\n",
    "                x, y = int(lndmrk_z.x * image.shape[1]), int(lndmrk_z.y * image.shape[0])\n",
    "                lndmrkX = lndmrk_z.x\n",
    "                lndmrkY = lndmrk_z.y\n",
    "                lndmrkZ = lndmrk_z.z\n",
    "\n",
    "            cv2.putText(\n",
    "                image,\n",
    "                f\"x: ({nose_z})\",\n",
    "                (50, 50),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                0.7,\n",
    "                (0, 255, 0),\n",
    "                2,\n",
    "                cv2.LINE_AA\n",
    "            )\n",
    "\n",
    "            cv2.imshow('Raw Webcam Feed', image)\n",
    "\n",
    "        except:\n",
    "            cv2.putText(\n",
    "                image,\n",
    "                f\"x: (No Detected)\",\n",
    "                (50, 50),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                0.7,\n",
    "                (0, 255, 0),\n",
    "                2,\n",
    "                cv2.LINE_AA\n",
    "            )\n",
    "\n",
    "            cv2.imshow('Raw Webcam Feed', image)\n",
    "\n",
    "        if cv2.waitKey(12) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3385f7e5",
   "metadata": {},
   "source": [
    "move forward goes till minus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fdce0eb",
   "metadata": {},
   "source": [
    "# Face Direction Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dc29e05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_direction(landmark):\n",
    "    nose = landmark[\"nose\"]\n",
    "    ear_r = landmark[\"ear_r\"]\n",
    "    ear_l = landmark[\"ear_l\"]\n",
    "\n",
    "    smallest_value = min(landmark, key=landmark.get)\n",
    "    highest_value = max(landmark, key=landmark.get)\n",
    "    \n",
    "    if smallest_value == \"nose\":\n",
    "        direction = \"Right\"\n",
    "    elif highest_value == \"nose\":\n",
    "        direction = \"Left\"\n",
    "    else:\n",
    "        direction = \"Center\"\n",
    "\n",
    "    return direction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96ee06f",
   "metadata": {},
   "source": [
    "# Camera Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "95c5aed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Camera is ready. Press 'q' to quit.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1756883084.383355  116717 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1756883084.415026  121328 gl_context.cc:369] GL version: 3.1 (OpenGL ES 3.1 Mesa 24.0.9-0ubuntu0.3), renderer: D3D12 (AMD Radeon(TM) Graphics)\n",
      "W0000 00:00:1756883084.898511  121318 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1756883084.972209  121323 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1756883084.986803  121316 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1756883084.988027  121320 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1756883084.994471  121324 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1756883085.012294  121320 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1756883085.017339  121315 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1756883085.032858  121325 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "# camera = cv2.VideoCapture(\"http://192.168.100.197:5000/video\")\n",
    "camera = cv2.VideoCapture(\"../test/vidio/record_1.mp4\")\n",
    "\n",
    "\n",
    "# Check if the camera opened successfully\n",
    "if not camera.isOpened():\n",
    "    print(\"Error: Could not open camera.\")\n",
    "else:\n",
    "    print(\"Camera is ready. Press 'q' to quit.\")\n",
    "\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.1, min_tracking_confidence=0.1) as holistic :\n",
    "    \n",
    "    while camera.isOpened():  \n",
    "        ret, frame = camera.read()\n",
    "\n",
    "        if not ret:\n",
    "            print(\"❌ Failed to grab frame\")\n",
    "            break\n",
    "        \n",
    "        # Recolor Feed\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False        \n",
    "        \n",
    "        # Make Detections\n",
    "        results = holistic.process(image)\n",
    "\n",
    "        # Recolor image back to BGR for rendering\n",
    "        image.flags.writeable = True   \n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        # =============== THE LANDMARKS ============== #\n",
    "        try:\n",
    "            nose = results.pose_landmarks.landmark[mp.solutions.holistic.PoseLandmark.NOSE]\n",
    "            ear_r = results.pose_landmarks.landmark[mp.solutions.holistic.PoseLandmark.RIGHT_EAR]\n",
    "            ear_l = results.pose_landmarks.landmark[mp.solutions.holistic.PoseLandmark.LEFT_EAR]\n",
    "            wrist_r = results.pose_landmarks.landmark[mp.solutions.holistic.PoseLandmark.RIGHT_WRIST]\n",
    "\n",
    "            # Array Each Landmark\n",
    "            face_direction = check_direction({\"nose\": nose.x, \"ear_r\": ear_r.x, \"ear_l\": ear_l.x}) \n",
    "\n",
    "            # For Draw\n",
    "            lm = landmark_pb2.NormalizedLandmarkList()\n",
    "            lm.landmark.extend([nose, ear_r, ear_l, wrist_r])\n",
    "\n",
    "            # ============================================ #\n",
    "\n",
    "            for lndmrk_z in lm.landmark:\n",
    "                x, y = int(lndmrk_z.x * image.shape[1]), int(lndmrk_z.y * image.shape[0])\n",
    "                cv2.circle(image, (x, y), 5, (255, 0, 0), -1)\n",
    "            \n",
    "                cv2.putText(\n",
    "                    image,\n",
    "                    f\"x: ({face_direction})\",\n",
    "                    (50, 50),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    0.7,\n",
    "                    (0, 255, 0),\n",
    "                    2,\n",
    "                    cv2.LINE_AA\n",
    "                )\n",
    "            # =============== DETECTION ================== #\n",
    "            # ============================================ #\n",
    "\n",
    "            cv2.imshow('Raw Webcam Feed', image)\n",
    "        except Exception as e:\n",
    "            \n",
    "\n",
    "            cv2.putText(\n",
    "                    image,\n",
    "                    f\"x: (Error {e})\",\n",
    "                    (50, 50),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    0.7,\n",
    "                    (0, 255, 0),\n",
    "                    2,\n",
    "                    cv2.LINE_AA\n",
    "                )\n",
    "            # =============== DETECTION ================== #\n",
    "            # ============================================ #\n",
    "\n",
    "            cv2.imshow('Raw Webcam Feed', image)\n",
    "\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "camera.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e014f80b",
   "metadata": {},
   "source": [
    "Left & Right Eye Can Be Used For the Left and Right"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d28761",
   "metadata": {},
   "source": [
    "# Body Rotation State"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852df1d4",
   "metadata": {},
   "source": [
    "### Getting State Of Body Direction Facing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e5d6fe24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def side_state(state, lndmrkX):\n",
    "    if state == 0:\n",
    "        if lndmrkX[\"nose\"] == min(lndmrkX.values()):\n",
    "            direction = \"Kanan\"\n",
    "        elif lndmrkX[\"nose\"] == max(lndmrkX.values()):      \n",
    "            direction = \"Kiri\"\n",
    "        else:\n",
    "            direction = \"Tengah\"\n",
    "    else:\n",
    "        if lndmrkX[\"nose\"] == min(lndmrkX.values()):\n",
    "            direction = \"Kiri\"\n",
    "        elif lndmrkX[\"nose\"] == max(lndmrkX.values()):      \n",
    "            direction = \"Kanan\"\n",
    "        else:\n",
    "            direction = \"Tengah\"\n",
    "    \n",
    "    return direction\n",
    "\n",
    "def hand_state(state, lndmrkZ):\n",
    "\n",
    "    smallest_value = min(lndmrkZ, key=lndmrkZ.get)\n",
    "    highest_value = max(lndmrkZ, key=lndmrkZ.get)\n",
    "\n",
    "    hand = smallest_value\n",
    "    wristR = f\"{lndmrkZ['wrist_r']:.2f}\"\n",
    "    nose = f\"{lndmrkZ['nose']:.2f}\"\n",
    "    \n",
    "    # return f\"WristR: {wristR} | Nose: {nose}\"\n",
    "\n",
    "    if state == 0:\n",
    "        if lndmrkZ[\"wrist_r\"] < lndmrkZ[\"nose\"]:\n",
    "            hand = \"Terlihat\"\n",
    "        else:\n",
    "            hand = \"Tidak Terlihat\"\n",
    "    else:\n",
    "        if lndmrkZ[\"wrist_r\"] > lndmrkZ[\"nose\"]:\n",
    "            hand = \"Terlihat\"\n",
    "        else:\n",
    "            hand = \"Tidak Terlihat\"\n",
    "    return hand\n",
    "\n",
    "def get_direction_state(lndmrk):\n",
    "    noseX, noseY, noseZ = lndmrk[\"nose\"].x, lndmrk[\"nose\"].y, lndmrk[\"nose\"].z\n",
    "    earLX, earLY, earLZ = lndmrk[\"ear_l\"].x, lndmrk[\"ear_l\"].y, lndmrk[\"ear_l\"].z\n",
    "    earRX, earRY, earRZ = lndmrk[\"ear_r\"].x, lndmrk[\"ear_r\"].y, lndmrk[\"ear_r\"].z\n",
    "    wristRX, wristRY, wristRZ = lndmrk[\"wrist_r\"].x, lndmrk[\"wrist_r\"].y, lndmrk[\"wrist_r\"].z\n",
    "    wristLX, wristLY, wristLZ = lndmrk[\"wrist_l\"].x, lndmrk[\"wrist_l\"].y, lndmrk[\"wrist_l\"].z\n",
    "    \n",
    "    lndmrkX = {\"nose\": noseX, \"ear_l\": earLX, \"ear_r\": earRX, \"wrist_r\": wristRX, \"wrist_l\": wristLX }\n",
    "    lndmrkY = {\"nose\": noseY, \"ear_l\": earLY, \"ear_r\": earRY, \"wrist_r\": wristRY, \"wrist_l\": wristLY }\n",
    "    lndmrkZ = {\"nose\": noseZ, \"ear_l\": earLZ, \"ear_r\": earRZ, \"wrist_r\": wristRZ, \"wrist_l\": wristLZ }\n",
    "\n",
    "    if noseZ < (earLZ and earRZ):\n",
    "        state = \"Hadap Depan\"\n",
    "        side = side_state(0, lndmrkX)\n",
    "        handZ = hand_state(0, lndmrkZ)\n",
    "    else:\n",
    "        state = \"Hadap Belakang\"\n",
    "        side = side_state(1, lndmrkX)\n",
    "        handZ = hand_state(1, lndmrkZ)\n",
    "\n",
    "    return f\"Tangan Z: {handZ}\"\n",
    "    return f\"{state} || Ke {side} || Tangan Z: {handZ}\"\n",
    "    # return f\"Z : {handZ}\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "95b125b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Camera is ready. Press 'q' to quit.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1756976647.946224     868 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1756976647.975640   40756 gl_context.cc:369] GL version: 3.1 (OpenGL ES 3.1 Mesa 24.0.9-0ubuntu0.3), renderer: D3D12 (AMD Radeon(TM) Graphics)\n",
      "W0000 00:00:1756976648.281368   40744 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1756976648.497435   40745 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1756976648.513253   40746 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1756976648.513814   40743 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1756976648.530477   40751 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1756976648.531395   40745 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1756976648.548978   40744 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1756976648.556562   40747 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "# camera = cv2.VideoCapture(\"http://192.168.100.197:5000/video\")\n",
    "camera = cv2.VideoCapture(\"../test/vidio/record_1.mp4\")\n",
    "\n",
    "# Check if the camera opened successfully\n",
    "if not camera.isOpened():\n",
    "    print(\"Error: Could not open camera.\")\n",
    "else:\n",
    "    print(\"Camera is ready. Press 'q' to quit.\")\n",
    "\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.3, min_tracking_confidence=0.1) as holistic :\n",
    "    \n",
    "    while camera.isOpened():  \n",
    "        ret, frame = camera.read()\n",
    "\n",
    "        if not ret:\n",
    "            print(\"❌ Failed to grab frame\")\n",
    "            break\n",
    "        \n",
    "        # Recolor Feed\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False        \n",
    "        \n",
    "        # Make Detections\n",
    "        results = holistic.process(image)\n",
    "\n",
    "        # Recolor image back to BGR for rendering\n",
    "        image.flags.writeable = True   \n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        # =============== THE LANDMARKS ============== #\n",
    "        try:\n",
    "            nose = results.pose_landmarks.landmark[mp.solutions.holistic.PoseLandmark.NOSE]\n",
    "            ear_r = results.pose_landmarks.landmark[mp.solutions.holistic.PoseLandmark.RIGHT_EAR]\n",
    "            ear_l = results.pose_landmarks.landmark[mp.solutions.holistic.PoseLandmark.LEFT_EAR]\n",
    "            wrist_r = results.pose_landmarks.landmark[mp.solutions.holistic.PoseLandmark.RIGHT_WRIST]\n",
    "            wrist_l = results.pose_landmarks.landmark[mp.solutions.holistic.PoseLandmark.LEFT_WRIST]\n",
    "\n",
    "            # Array Each Landmark\n",
    "            state_direction = get_direction_state({\"nose\": nose, \"ear_l\": ear_l, \"ear_r\": ear_r, \"wrist_r\": wrist_r, \"wrist_l\": wrist_l})\n",
    "\n",
    "            # For Draw\n",
    "            lm = landmark_pb2.NormalizedLandmarkList()\n",
    "            lm.landmark.extend([nose, ear_r, ear_l, wrist_r])\n",
    "\n",
    "            # ============================================ #\n",
    "\n",
    "            for lndmrk_z in lm.landmark:\n",
    "                x, y = int(lndmrk_z.x * image.shape[1]), int(lndmrk_z.y * image.shape[0])\n",
    "                cv2.circle(image, (x, y), 5, (255, 0, 0), -1)\n",
    "            \n",
    "                cv2.putText(\n",
    "                    image,\n",
    "                    f\"x: ({state_direction})\",\n",
    "                    (50, 50),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    0.7,\n",
    "                    (0, 255, 0),\n",
    "                    2,\n",
    "                    cv2.LINE_AA\n",
    "                )\n",
    "            # =============== DETECTION ================== #\n",
    "            # ============================================ #\n",
    "\n",
    "            cv2.imshow('Raw Webcam Feed', image)\n",
    "        except Exception as e:\n",
    "            \n",
    "\n",
    "            cv2.putText(\n",
    "                    image,\n",
    "                    f\"x: (Error {e})\",\n",
    "                    (50, 50),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    0.7,\n",
    "                    (0, 255, 0),\n",
    "                    2,\n",
    "                    cv2.LINE_AA\n",
    "                )\n",
    "            \n",
    "            # =============== DETECTION ================== #\n",
    "            # ============================================ #\n",
    "\n",
    "            cv2.imshow('Raw Webcam Feed', image)\n",
    "\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "camera.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc277b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2b8021",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "motion_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
