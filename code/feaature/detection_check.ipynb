{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361f6f1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe59f71c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/c/users/nawfal/documents/apps/collaborative project/cctv_motion_detection/motion_venv/bin/python'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cdac84c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f5d578e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cap = cv2.VideoCapture(\"http://192.168.50.234:5000/video\")\n",
    "cap = cv2.VideoCapture(\"http://192.168.100.197:5000/video\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa3111a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the camera\n",
    "\n",
    "# Check if the camera opened successfully\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open camera.\")\n",
    "else:\n",
    "    print(\"Camera is ready. Press 'q' to quit.\")\n",
    "\n",
    "# Capture frames from the camera\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to grab frame.\")\n",
    "        break\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.namedWindow(\"Camera Capture\", cv2.WINDOW_NORMAL | cv2.WINDOW_GUI_NORMAL)\n",
    "    cv2.resizeWindow(\"Camera Capture\", 640, 480)\n",
    "    cv2.imshow(\"Camera Capture\", frame)\n",
    "\n",
    "    # Exit the loop when 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the camera and close all OpenCV windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9697c8",
   "metadata": {},
   "source": [
    "# Import Mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a7ade97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import time\n",
    "import csv\n",
    "import warnings\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import mediapipe as mp\n",
    "from mediapipe.framework.formats import landmark_pb2\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils # Drawing helpers\n",
    "mp_holistic = mp.solutions.holistic # Mediapipe Solutions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230d207b",
   "metadata": {},
   "source": [
    "# Z Coordinate detection\n",
    "Zoom in & Zoom Out while move forward and backwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdeb05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lndmrkX = None\n",
    "lndmrkY = None\n",
    "lndmrkZ = None\n",
    "\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic :\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Recolor Feed\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False        \n",
    "        \n",
    "        # Make Detections\n",
    "        results = holistic.process(image)\n",
    "\n",
    "        # Recolor image back to BGR for rendering\n",
    "        image.flags.writeable = True   \n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        # Get specific landmarks\n",
    "        nose = results.pose_landmarks.landmark[mp.solutions.holistic.PoseLandmark.NOSE]\n",
    "\n",
    "        show_landmark_list = landmark_pb2.NormalizedLandmarkList()\n",
    "        show_landmark_list.landmark.extend([nose])\n",
    "\n",
    "        # Draw landmarks\n",
    "        for landmark in show_landmark_list.landmark:\n",
    "            x, y = int(landmark.x * image.shape[1]), int(landmark.y * image.shape[0])\n",
    "            lndmrkX = landmark.x\n",
    "            lndmrkY = landmark.y\n",
    "            lndmrkZ = landmark.z\n",
    "\n",
    "        cv2.circle(image, (x, y), 5, (255, 0, 0), -1)\n",
    "\n",
    "        cv2.putText(image,\n",
    "            f\"Nose: (\\nz: {lndmrkZ:.2f})\",\n",
    "            (x + 10, y - 10),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            0.7,               # font scale\n",
    "            (0, 255, 0),       # color\n",
    "            2,                 # thickness\n",
    "            cv2.LINE_AA)\n",
    "        cv2.imshow('Raw Webcam Feed', image)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3385f7e5",
   "metadata": {},
   "source": [
    "move forward goes till minus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fdce0eb",
   "metadata": {},
   "source": [
    "# Face Direction Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc29e05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_direction(landmark):\n",
    "    direction = \"Right\"\n",
    "    smallest_value = min(landmark, key=landmark.get)\n",
    "    highest_value = max(landmark, key=landmark.get)\n",
    "    \n",
    "    if smallest_value == \"nose\":\n",
    "        direction = \"Right\"\n",
    "    elif highest_value == \"nose\":\n",
    "        direction = \"Left\"\n",
    "    else:\n",
    "        direction = \"Center\"\n",
    "    \n",
    "    return direction\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "95c5aed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Camera is ready. Press 'q' to quit.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1755499288.940323  160903 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1755499288.970242  174902 gl_context.cc:369] GL version: 3.1 (OpenGL ES 3.1 Mesa 24.0.9-0ubuntu0.3), renderer: D3D12 (AMD Radeon(TM) Graphics)\n",
      "W0000 00:00:1755499289.563085  174891 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1755499289.642083  174893 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1755499289.654893  174891 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1755499289.660571  174898 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1755499289.661942  174897 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1755499289.684009  174889 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1755499289.685592  174893 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1755499289.699457  174896 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "camera = cv2.VideoCapture(\"http://192.168.100.197:5000/video\")\n",
    "\n",
    "# Check if the camera opened successfully\n",
    "if not camera.isOpened():\n",
    "    print(\"Error: Could not open camera.\")\n",
    "else:\n",
    "    print(\"Camera is ready. Press 'q' to quit.\")\n",
    "\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.1, min_tracking_confidence=0.1) as holistic :\n",
    "    \n",
    "    while camera.isOpened():  \n",
    "        ret, frame = camera.read()\n",
    "\n",
    "        if not ret:\n",
    "            print(\"‚ùå Failed to grab frame\")\n",
    "            break\n",
    "        \n",
    "        # Recolor Feed\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False        \n",
    "        \n",
    "        # Make Detections\n",
    "        results = holistic.process(image)\n",
    "\n",
    "        # Recolor image back to BGR for rendering\n",
    "        image.flags.writeable = True   \n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        # =============== THE LANDMARKS ============== #\n",
    "        \n",
    "        nose = results.pose_landmarks.landmark[mp.solutions.holistic.PoseLandmark.NOSE]\n",
    "        ear_r = results.pose_landmarks.landmark[mp.solutions.holistic.PoseLandmark.RIGHT_EAR]\n",
    "        ear_l = results.pose_landmarks.landmark[mp.solutions.holistic.PoseLandmark.LEFT_EAR]\n",
    "        wrist_r = results.pose_landmarks.landmark[mp.solutions.holistic.PoseLandmark.RIGHT_WRIST]\n",
    "\n",
    "        # Array Each Landmark\n",
    "        face_direction = check_direction({\"nose\": nose.x, \"ear_r\": ear_r.x, \"ear_l\": ear_l.x}) \n",
    "\n",
    "        # For Draw\n",
    "        lm = landmark_pb2.NormalizedLandmarkList()\n",
    "        lm.landmark.extend([nose, ear_r, ear_l, wrist_r])\n",
    "\n",
    "        # ============================================ #\n",
    "\n",
    "        for landmark in lm.landmark:\n",
    "            x, y = int(landmark.x * image.shape[1]), int(landmark.y * image.shape[0])\n",
    "            cv2.circle(image, (x, y), 5, (255, 0, 0), -1)\n",
    "        \n",
    "        cv2.putText(image,\n",
    "            # f\"x: ({landmark.x:.2f})\",\n",
    "            f\"x: ({face_direction})\",\n",
    "            (x + 10, y - 10),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            0.7,               # font scale\n",
    "            (0, 255, 0),       # color\n",
    "            2,                 # thickness\n",
    "            cv2.LINE_AA)\n",
    "\n",
    "        # =============== DETECTION ================== #\n",
    "        # ============================================ #\n",
    "\n",
    "        cv2.imshow('Raw Webcam Feed', image)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "camera.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e014f80b",
   "metadata": {},
   "source": [
    "Left & Right Eye Can Be Used For the Left and Right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d28761",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "motion_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
