{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7c834c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 20:57:16.309383: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1755003436.343580    6525 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1755003436.352427    6525 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1755003436.383617    6525 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1755003436.383653    6525 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1755003436.383655    6525 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1755003436.383657    6525 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-08-12 20:57:16.393836: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import time\n",
    "import csv\n",
    "import warnings\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import mediapipe as mp\n",
    "from mediapipe.framework.formats import landmark_pb2\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils # Drawing helpers\n",
    "mp_holistic = mp.solutions.holistic # Mediapipe Solutions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09a2bdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "desire_fps = 15\n",
    "num_frames = 15\n",
    "\n",
    "original_width, original_height = 500, 500\n",
    "small_width, small_height = 150, 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f70d4ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../model/v1_model.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d64cee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Camera is ready. Press 'q' to quit.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1755003557.304823    6525 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1755003557.343437    7089 gl_context.cc:369] GL version: 3.1 (OpenGL ES 3.1 Mesa 24.0.9-0ubuntu0.3), renderer: D3D12 (AMD Radeon(TM) Graphics)\n",
      "W0000 00:00:1755003557.800296    7078 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1755003558.059545    7079 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1755003558.074319    7084 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1755003558.074850    7077 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1755003558.084129    7080 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1755003558.112143    7081 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1755003558.139990    7078 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1755003558.150503    7076 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "camera = cv2.VideoCapture(\"http://192.168.50.234:5000/video\")\n",
    "\n",
    "frames = []\n",
    "detected = []\n",
    "\n",
    "if not camera.isOpened():\n",
    "    print(\"Error: Could not open camera.\")\n",
    "else:\n",
    "    print(\"Camera is ready. Press 'q' to quit.\")\n",
    "\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    start_time = time.time()\n",
    "\n",
    "    while camera.isOpened():\n",
    "        ret, frame = camera.read()\n",
    "\n",
    "        if ret:\n",
    "\n",
    "            frame = cv2.resize(frame, (original_width, original_height))\n",
    "\n",
    "            if len(frames) < num_frames:\n",
    "                frames.append(cv2.resize(frame, (small_width, small_height)))\n",
    "\n",
    "                frame_detected = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                frame_detected.flags.writeable = False\n",
    "\n",
    "                results = holistic.process(frame_detected)\n",
    "\n",
    "                frame_detected.flags.writeable = True\n",
    "                frame_detected = cv2.cvtColor(frame_detected, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "                temporary_list = []\n",
    "                temporary_list.append(results.pose_landmarks.landmark[mp.solutions.holistic.PoseLandmark.NOSE])\n",
    "                temporary_list.append(results.pose_landmarks.landmark[mp.solutions.holistic.PoseLandmark.LEFT_EYE])\n",
    "                temporary_list.append(results.pose_landmarks.landmark[mp.solutions.holistic.PoseLandmark.RIGHT_EYE])\n",
    "                \n",
    "                detected.append(temporary_list)\n",
    "            else:\n",
    "                frames.pop(0)\n",
    "                detected.pop(0)\n",
    "\n",
    "                frames.append(cv2.resize(frame, (small_width, small_height)))\n",
    "\n",
    "                frame_detected = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                frame_detected.flags.writeable = False\n",
    "\n",
    "                results = holistic.process(frame_detected)\n",
    "                \n",
    "                frame_detected.flags.writeable = True\n",
    "                frame_detected = cv2.cvtColor(frame_detected, cv2.COLOR_RGB2BGR)\n",
    "                \n",
    "                temporary_list = []\n",
    "                temporary_list.append(results.pose_landmarks.landmark[mp.solutions.holistic.PoseLandmark.NOSE])\n",
    "                temporary_list.append(results.pose_landmarks.landmark[mp.solutions.holistic.PoseLandmark.LEFT_EYE])\n",
    "                temporary_list.append(results.pose_landmarks.landmark[mp.solutions.holistic.PoseLandmark.RIGHT_EYE])\n",
    "                \n",
    "                detected.append(temporary_list)\n",
    "            \n",
    "            if len(frames) >= num_frames:\n",
    "                stacked1_frames = np.vstack((frames[0], frames[1], frames[2]))\n",
    "                stacked2_frames = np.vstack((frames[3], frames[4], frames[5]))\n",
    "                stacked3_frames = np.vstack((frames[6], frames[7], frames[8]))\n",
    "                stacked4_frames = np.vstack((frames[9], frames[10], frames[11]))\n",
    "                stacked5_frames = np.vstack((frames[12], frames[13], frames[14]))\n",
    "\n",
    "                stacked1_frames = cv2.resize(stacked1_frames, (small_width, original_height))\n",
    "                stacked2_frames = cv2.resize(stacked2_frames, (small_width, original_height))\n",
    "                stacked3_frames = cv2.resize(stacked3_frames, (small_width, original_height))\n",
    "                stacked4_frames = cv2.resize(stacked4_frames, (small_width, original_height))\n",
    "                stacked5_frames = cv2.resize(stacked5_frames, (small_width, original_height))\n",
    "                \n",
    "                final_layout = np.hstack((frame, stacked1_frames, stacked2_frames, stacked3_frames, stacked4_frames, stacked5_frames))\n",
    "            else:\n",
    "                final_layout = frame\n",
    "\n",
    "            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            image.flags.writeable = False\n",
    "\n",
    "            flatten_detected = [landmark for sublist in detected for landmark in sublist]\n",
    "\n",
    "            landmark_list = landmark_pb2.NormalizedLandmarkList()\n",
    "            landmark_list.landmark.extend(flatten_detected)\n",
    "\n",
    "\n",
    "            try:\n",
    "                lv = landmark_list.landmark\n",
    "                motion_row = list(np.array([[landmark.x, landmark.y, landmark.z, landmark.visibility] for landmark in lv]).flatten()) \n",
    "                \n",
    "                motion_detected = pd.DataFrame([motion_row])\n",
    "                motion_class = model.predict(motion_detected)[0]\n",
    "\n",
    "                warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"sklearn\")\n",
    "                \n",
    "                cv2.putText(final_layout, f'Class: {motion_class}', (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "            except Exception as e:\n",
    "                pass\n",
    "\n",
    "\n",
    "            cv2.imshow(\"Camera Feed\", final_layout)\n",
    "            \n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "        \n",
    "camera.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6e803d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "motion_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
