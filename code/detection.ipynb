{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f7c834c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import time\n",
    "import csv\n",
    "import warnings\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import mediapipe as mp\n",
    "from mediapipe.framework.formats import landmark_pb2\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils # Drawing helpers\n",
    "mp_holistic = mp.solutions.holistic # Mediapipe Solutions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "09a2bdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "desire_fps = 15\n",
    "num_frames = 15\n",
    "\n",
    "original_width, original_height = 500, 500\n",
    "small_width, small_height = 150, 150"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ac795f",
   "metadata": {},
   "source": [
    "# V3 MODEL DETECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f70d4ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../model/v3_model.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fac92464",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_direction(landmark):\n",
    "    direction = \"Right\"\n",
    "    smallest_value = min(landmark, key=landmark.get)\n",
    "    highest_value = max(landmark, key=landmark.get)\n",
    "    \n",
    "    if smallest_value == \"nose\":\n",
    "        direction = 0\n",
    "    elif highest_value == \"nose\":\n",
    "        direction = 1\n",
    "    else:\n",
    "        direction = 2\n",
    "    \n",
    "    return direction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d64cee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Camera is ready. Press 'q' to quit.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MESA: error: ZINK: failed to choose pdev\n",
      "libEGL warning: egl: failed to create dri2 screen\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1757088183.170217   98610 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1757088183.225542   99293 gl_context.cc:369] GL version: 3.1 (OpenGL ES 3.1 Mesa 24.0.9-0ubuntu0.3), renderer: D3D12 (AMD Radeon(TM) Graphics)\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1757088183.821393   99273 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1757088184.168477   99274 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1757088184.191760   99273 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n",
      "W0000 00:00:1757088184.197273   99282 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1757088184.197278   99281 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1757088184.206524   99280 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1757088184.229784   99275 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1757088184.247897   99278 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1757088184.252538   99276 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "/mnt/c/users/nawfal/documents/apps/collaborative project/cctv_motion_detection/motion_venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "/mnt/c/users/nawfal/documents/apps/collaborative project/cctv_motion_detection/motion_venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "/mnt/c/users/nawfal/documents/apps/collaborative project/cctv_motion_detection/motion_venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "/mnt/c/users/nawfal/documents/apps/collaborative project/cctv_motion_detection/motion_venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "/mnt/c/users/nawfal/documents/apps/collaborative project/cctv_motion_detection/motion_venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "/mnt/c/users/nawfal/documents/apps/collaborative project/cctv_motion_detection/motion_venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "/mnt/c/users/nawfal/documents/apps/collaborative project/cctv_motion_detection/motion_venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "/mnt/c/users/nawfal/documents/apps/collaborative project/cctv_motion_detection/motion_venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "/mnt/c/users/nawfal/documents/apps/collaborative project/cctv_motion_detection/motion_venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "/mnt/c/users/nawfal/documents/apps/collaborative project/cctv_motion_detection/motion_venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "/mnt/c/users/nawfal/documents/apps/collaborative project/cctv_motion_detection/motion_venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "/mnt/c/users/nawfal/documents/apps/collaborative project/cctv_motion_detection/motion_venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "/mnt/c/users/nawfal/documents/apps/collaborative project/cctv_motion_detection/motion_venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "/mnt/c/users/nawfal/documents/apps/collaborative project/cctv_motion_detection/motion_venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "/mnt/c/users/nawfal/documents/apps/collaborative project/cctv_motion_detection/motion_venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "camera = cv2.VideoCapture(\"http://192.168.50.234:5000/video\")\n",
    "\n",
    "frames = []\n",
    "detected = []\n",
    "face_direction = []\n",
    "\n",
    "desire_fps = 5\n",
    "frame_delay = 1.0 / desire_fps\n",
    "\n",
    "if not camera.isOpened():\n",
    "    print(\"Error: Could not open camera.\")\n",
    "else:\n",
    "    print(\"Camera is ready. Press 'q' to quit.\")\n",
    "\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    start_time = time.time()\n",
    "\n",
    "    while camera.isOpened():\n",
    "        delay_frame_time = time.time()\n",
    "        ret, frame = camera.read()\n",
    "\n",
    "        if ret:\n",
    "\n",
    "            frame = cv2.resize(frame, (original_width, original_height))\n",
    "\n",
    "            if len(frames) < num_frames:\n",
    "                frames.append(cv2.resize(frame, (small_width, small_height)))\n",
    "\n",
    "                frame_detected = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                frame_detected.flags.writeable = False\n",
    "\n",
    "                results = holistic.process(frame_detected)\n",
    "\n",
    "                frame_detected.flags.writeable = True\n",
    "                frame_detected = cv2.cvtColor(frame_detected, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "                nose = results.pose_landmarks.landmark[mp.solutions.holistic.PoseLandmark.NOSE]\n",
    "                ear_r = results.pose_landmarks.landmark[mp.solutions.holistic.PoseLandmark.RIGHT_EAR]\n",
    "                ear_l = results.pose_landmarks.landmark[mp.solutions.holistic.PoseLandmark.LEFT_EAR]\n",
    "                wrist_r = results.pose_landmarks.landmark[mp.solutions.holistic.PoseLandmark.RIGHT_WRIST]\n",
    "                wrist_l = results.pose_landmarks.landmark[mp.solutions.holistic.PoseLandmark.RIGHT_WRIST]\n",
    "\n",
    "                temporary_list = []\n",
    "                temporary_list.append(nose)\n",
    "                temporary_list.append(ear_l)\n",
    "                temporary_list.append(ear_r)\n",
    "                temporary_list.append(wrist_r)\n",
    "                temporary_list.append(wrist_l)\n",
    "                \n",
    "                face_direction.append(check_direction({\"nose\": nose.x, \"ear_r\": ear_r.x, \"ear_l\": ear_l.x}))\n",
    "                detected.append(temporary_list)\n",
    "            else:\n",
    "                frames.pop(0)\n",
    "                face_direction.pop(0)\n",
    "                detected.pop(0)\n",
    "\n",
    "                frames.append(cv2.resize(frame, (small_width, small_height)))\n",
    "\n",
    "                frame_detected = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                frame_detected.flags.writeable = False\n",
    "\n",
    "                results = holistic.process(frame_detected)\n",
    "                \n",
    "                frame_detected.flags.writeable = True\n",
    "                frame_detected = cv2.cvtColor(frame_detected, cv2.COLOR_RGB2BGR)\n",
    "                \n",
    "                nose = results.pose_landmarks.landmark[mp.solutions.holistic.PoseLandmark.NOSE]\n",
    "                ear_r = results.pose_landmarks.landmark[mp.solutions.holistic.PoseLandmark.RIGHT_EAR]\n",
    "                ear_l = results.pose_landmarks.landmark[mp.solutions.holistic.PoseLandmark.LEFT_EAR]\n",
    "                wrist_r = results.pose_landmarks.landmark[mp.solutions.holistic.PoseLandmark.RIGHT_WRIST]\n",
    "                wrist_l = results.pose_landmarks.landmark[mp.solutions.holistic.PoseLandmark.RIGHT_WRIST]\n",
    "                \n",
    "                temporary_list = []\n",
    "                temporary_list.append(nose)\n",
    "                temporary_list.append(ear_l)\n",
    "                temporary_list.append(ear_r)\n",
    "                temporary_list.append(wrist_r)\n",
    "                temporary_list.append(wrist_l)\n",
    "                \n",
    "                face_direction.append(check_direction({\"nose\": nose.x, \"ear_r\": ear_r.x, \"ear_l\": ear_l.x}))\n",
    "                detected.append(temporary_list)\n",
    "            \n",
    "            if len(frames) >= num_frames:\n",
    "                stacked1_frames = np.vstack((frames[0], frames[1], frames[2]))\n",
    "                stacked2_frames = np.vstack((frames[3], frames[4], frames[5]))\n",
    "                stacked3_frames = np.vstack((frames[6], frames[7], frames[8]))\n",
    "                stacked4_frames = np.vstack((frames[9], frames[10], frames[11]))\n",
    "                stacked5_frames = np.vstack((frames[12], frames[13], frames[14]))\n",
    "\n",
    "                stacked1_frames = cv2.resize(stacked1_frames, (small_width, original_height))\n",
    "                stacked2_frames = cv2.resize(stacked2_frames, (small_width, original_height))\n",
    "                stacked3_frames = cv2.resize(stacked3_frames, (small_width, original_height))\n",
    "                stacked4_frames = cv2.resize(stacked4_frames, (small_width, original_height))\n",
    "                stacked5_frames = cv2.resize(stacked5_frames, (small_width, original_height))\n",
    "                \n",
    "                final_layout = np.hstack((frame, stacked1_frames, stacked2_frames, stacked3_frames, stacked4_frames, stacked5_frames))\n",
    "            else:\n",
    "                final_layout = frame\n",
    "\n",
    "            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            image.flags.writeable = False\n",
    "\n",
    "            flatten_detected = [landmark for sublist in detected for landmark in sublist]\n",
    "\n",
    "            landmark_list = landmark_pb2.NormalizedLandmarkList()\n",
    "            landmark_list.landmark.extend(flatten_detected)\n",
    "\n",
    "\n",
    "            try:\n",
    "                lv = landmark_list.landmark\n",
    "                counter = 0\n",
    "                featureper_frames = 0\n",
    "                motion_row = []\n",
    "                \n",
    "                for lndmrk in lv:\n",
    "                    motion_row.append(lndmrk.x)\n",
    "                    motion_row.append(lndmrk.y)\n",
    "                    motion_row.append(lndmrk.z)\n",
    "                    motion_row.append(lndmrk.visibility)\n",
    "                    counter += 1\n",
    "\n",
    "                    if counter % 5 == 0:\n",
    "                        motion_row.append(face_direction[featureper_frames])\n",
    "                        featureper_frames += 1 \n",
    "\n",
    "                motion_row = list(np.array(motion_row))\n",
    "                \n",
    "                motion_detected = pd.DataFrame([motion_row])\n",
    "                motion_class = model.predict(motion_detected)[0]\n",
    "\n",
    "                warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"sklearn\")\n",
    "                \n",
    "                cv2.putText(final_layout, f'Class: {motion_class}', (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "            except Exception as e:\n",
    "                pass\n",
    "\n",
    "\n",
    "            cv2.imshow(\"Camera Feed\", final_layout)\n",
    "            \n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "        \n",
    "camera.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30a17f1",
   "metadata": {},
   "source": [
    "# V4 MODEL DETECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "767e8da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../model/v4_model.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6a7da3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def side_state(state, lndmrkX):\n",
    "    if state == 0:\n",
    "        if lndmrkX[\"nose\"] == min(lndmrkX.values()):\n",
    "            direction = 1 # Kiri\n",
    "        elif lndmrkX[\"nose\"] == max(lndmrkX.values()):      \n",
    "            direction = 2 # Kanan\n",
    "        else:\n",
    "            direction = 0 # Tengah\n",
    "    else:\n",
    "        if lndmrkX[\"nose\"] == min(lndmrkX.values()):\n",
    "            direction = 2 # Kanan\n",
    "        elif lndmrkX[\"nose\"] == max(lndmrkX.values()):      \n",
    "            direction = 1 # Kiri\n",
    "        else:\n",
    "            direction = 0 # Tengah\n",
    "    \n",
    "    return direction\n",
    "\n",
    "def hand_state(state, lndmrkZ):\n",
    "    if state == 0:\n",
    "        if lndmrkZ[\"wrist_r\"] and lndmrkZ[\"wrist_l\"] < lndmrkZ[\"nose\"]:\n",
    "            hand = 1 # Terlihat\n",
    "        else:\n",
    "            hand = 0 # Tidak Terlihat\n",
    "    else:\n",
    "        if lndmrkZ[\"wrist_r\"] and lndmrkZ[\"wrist_l\"] < lndmrkZ[\"nose\"]:\n",
    "            hand = 0 # Terlihat\n",
    "        else:\n",
    "            hand = 1 # Tidak Terlihat\n",
    "    return hand\n",
    "\n",
    "def get_extFeature_value(lndmrk):\n",
    "    noseX, noseY, noseZ = lndmrk[\"nose\"].x, lndmrk[\"nose\"].y, lndmrk[\"nose\"].z\n",
    "    earLX, earLY, earLZ = lndmrk[\"ear_l\"].x, lndmrk[\"ear_l\"].y, lndmrk[\"ear_l\"].z\n",
    "    earRX, earRY, earRZ = lndmrk[\"ear_r\"].x, lndmrk[\"ear_r\"].y, lndmrk[\"ear_r\"].z\n",
    "    wristRX, wristRY, wristRZ = lndmrk[\"wrist_r\"].x, lndmrk[\"wrist_r\"].y, lndmrk[\"wrist_r\"].z\n",
    "    wristLX, wristLY, wristLZ = lndmrk[\"wrist_l\"].x, lndmrk[\"wrist_l\"].y, lndmrk[\"wrist_l\"].z\n",
    "    \n",
    "    lndmrkX = {\"nose\": noseX, \"ear_l\": earLX, \"ear_r\": earRX, \"wrist_r\": wristRX, \"wrist_l\": wristLX }\n",
    "    lndmrkY = {\"nose\": noseY, \"ear_l\": earLY, \"ear_r\": earRY, \"wrist_r\": wristRY, \"wrist_l\": wristLY }\n",
    "    lndmrkZ = {\"nose\": noseZ, \"ear_l\": earLZ, \"ear_r\": earRZ, \"wrist_r\": wristRZ, \"wrist_l\": wristLZ }\n",
    "\n",
    "    if noseZ < (earLZ and earRZ):\n",
    "        state = 0\n",
    "        side = side_state(0, lndmrkX)\n",
    "        hand = hand_state(0, lndmrkZ)\n",
    "    else:\n",
    "        state = 1\n",
    "        side = side_state(1, lndmrkX)\n",
    "        hand = hand_state(1, lndmrkZ)\n",
    "\n",
    "    return side, state, hand\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e61f04c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Camera is ready. Press 'q' to quit.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1757090063.938961  100551 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1757090063.976465  106227 gl_context.cc:369] GL version: 3.1 (OpenGL ES 3.1 Mesa 24.0.9-0ubuntu0.3), renderer: D3D12 (AMD Radeon(TM) Graphics)\n",
      "W0000 00:00:1757090064.388000  106216 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1757090064.555985  106217 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1757090064.570517  106221 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1757090064.571134  106218 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1757090064.577727  106224 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1757090064.583200  106216 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1757090064.618516  106220 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1757090064.627523  106222 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error : X has 23 features, but StandardScaler is expecting 345 features as input.\n",
      "Error : X has 46 features, but StandardScaler is expecting 345 features as input.\n",
      "Error : X has 69 features, but StandardScaler is expecting 345 features as input.\n",
      "Error : X has 92 features, but StandardScaler is expecting 345 features as input.\n",
      "Error : X has 115 features, but StandardScaler is expecting 345 features as input.\n",
      "Error : X has 138 features, but StandardScaler is expecting 345 features as input.\n",
      "Error : X has 161 features, but StandardScaler is expecting 345 features as input.\n",
      "Error : X has 184 features, but StandardScaler is expecting 345 features as input.\n",
      "Error : X has 207 features, but StandardScaler is expecting 345 features as input.\n",
      "Error : X has 230 features, but StandardScaler is expecting 345 features as input.\n",
      "Error : X has 253 features, but StandardScaler is expecting 345 features as input.\n",
      "Error : X has 276 features, but StandardScaler is expecting 345 features as input.\n",
      "Error : X has 299 features, but StandardScaler is expecting 345 features as input.\n",
      "Error : X has 322 features, but StandardScaler is expecting 345 features as input.\n"
     ]
    }
   ],
   "source": [
    "camera = cv2.VideoCapture(\"test/vidio/record_nm.mp4\")\n",
    "\n",
    "frames = []\n",
    "detected = []\n",
    "\n",
    "# ==== EXTENDED FEATURE ====\n",
    "face_direction = []\n",
    "face_shown = []\n",
    "hand_shown = []\n",
    "\n",
    "desire_fps = 5\n",
    "frame_delay = 1.0 / desire_fps\n",
    "\n",
    "saved = 0\n",
    "\n",
    "if not camera.isOpened():\n",
    "    print(\"Error: Could not open camera.\")\n",
    "else:\n",
    "    print(\"Camera is ready. Press 'q' to quit.\")\n",
    "\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.1, min_tracking_confidence=0.1) as holistic:\n",
    "    start_time = time.time()\n",
    "    \n",
    "    while camera.isOpened:\n",
    "        delay_frame_time = time.time()\n",
    "        countdown_time = 1\n",
    "        \n",
    "        ret, frame = camera.read()\n",
    "        \n",
    "        if ret:\n",
    "\n",
    "            frame = cv2.resize(frame, (original_width, original_height))\n",
    "\n",
    "            if len(frames) < num_frames:\n",
    "                try:\n",
    "                    frames.append(cv2.resize(frame, (small_width, small_height)))\n",
    "                    \n",
    "                    frame_detected = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                    frame_detected.flags.writeable = False\n",
    "                    results = holistic.process(frame_detected)\n",
    "                    frame_detected.flags.writeable = True\n",
    "                    frame_detected = cv2.cvtColor(frame_detected, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "                    nose = results.pose_landmarks.landmark[mp.solutions.holistic.PoseLandmark.NOSE]\n",
    "                    ear_r = results.pose_landmarks.landmark[mp.solutions.holistic.PoseLandmark.RIGHT_EAR]\n",
    "                    ear_l = results.pose_landmarks.landmark[mp.solutions.holistic.PoseLandmark.LEFT_EAR]\n",
    "                    wrist_r = results.pose_landmarks.landmark[mp.solutions.holistic.PoseLandmark.RIGHT_WRIST]\n",
    "                    wrist_l = results.pose_landmarks.landmark[mp.solutions.holistic.PoseLandmark.RIGHT_WRIST]\n",
    "\n",
    "                    temporary_list = []\n",
    "                    temporary_list.append(nose)\n",
    "                    temporary_list.append(ear_l)\n",
    "                    temporary_list.append(ear_r)\n",
    "                    temporary_list.append(wrist_r)\n",
    "                    temporary_list.append(wrist_l)\n",
    "                    \n",
    "                    extended_feature = get_extFeature_value({\"nose\": nose, \"ear_r\": ear_r, \"ear_l\": ear_l, \"wrist_r\": wrist_r, \"wrist_l\": wrist_l})\n",
    "                    face_direction.append(extended_feature[0])\n",
    "                    face_shown.append(extended_feature[1])\n",
    "                    hand_shown.append(extended_feature[2])\n",
    "\n",
    "                    detected.append(temporary_list)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    error_text = f\"No detection: {e}\"\n",
    "                    # cv2.putText(frame, error_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "                \n",
    "            else:\n",
    "                try:\n",
    "                    frames.append(cv2.resize(frame, (small_width, small_height)))\n",
    "\n",
    "                    frame_detected = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                    frame_detected.flags.writeable = False\n",
    "                    results = holistic.process(frame_detected)\n",
    "                    frame_detected.flags.writeable = True\n",
    "                    frame_detected = cv2.cvtColor(frame_detected, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "                    nose = results.pose_landmarks.landmark[mp.solutions.holistic.PoseLandmark.NOSE]\n",
    "                    ear_r = results.pose_landmarks.landmark[mp.solutions.holistic.PoseLandmark.RIGHT_EAR]\n",
    "                    ear_l = results.pose_landmarks.landmark[mp.solutions.holistic.PoseLandmark.LEFT_EAR]\n",
    "                    wrist_r = results.pose_landmarks.landmark[mp.solutions.holistic.PoseLandmark.RIGHT_WRIST]\n",
    "                    wrist_l = results.pose_landmarks.landmark[mp.solutions.holistic.PoseLandmark.RIGHT_WRIST]\n",
    "                    \n",
    "                    temporary_list = []\n",
    "                    temporary_list.append(nose)\n",
    "                    temporary_list.append(ear_l)\n",
    "                    temporary_list.append(ear_r)\n",
    "                    temporary_list.append(wrist_r)\n",
    "                    temporary_list.append(wrist_l)\n",
    "\n",
    "                    extended_feature = get_extFeature_value({\"nose\": nose, \"ear_r\": ear_r, \"ear_l\": ear_l, \"wrist_r\": wrist_r, \"wrist_l\": wrist_l})\n",
    "                    face_direction.append(extended_feature[0])\n",
    "                    face_shown.append(extended_feature[1])\n",
    "                    hand_shown.append(extended_feature[2])\n",
    "\n",
    "                    frames.pop(0)\n",
    "\n",
    "                    face_direction.pop(0)\n",
    "                    face_shown.pop(0)\n",
    "                    hand_shown.pop(0)\n",
    "\n",
    "                    detected.pop(0)\n",
    "\n",
    "                    detected.append(temporary_list)\n",
    "                except Exception as e:\n",
    "                    error_text = f\"No detection: {e}\"\n",
    "                    # cv2.putText(frame, error_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "            if len(frames) >= 15:\n",
    "                stacked1_frames = np.vstack((frames[0], frames[1], frames[2]))\n",
    "                stacked2_frames = np.vstack((frames[3], frames[4], frames[5]))\n",
    "                stacked3_frames = np.vstack((frames[6], frames[7], frames[8]))\n",
    "                stacked4_frames = np.vstack((frames[9], frames[10], frames[11]))\n",
    "                stacked5_frames = np.vstack((frames[12], frames[13], frames[14]))\n",
    "\n",
    "                stacked1_frames = cv2.resize(stacked1_frames, (small_width, original_height))\n",
    "                stacked2_frames = cv2.resize(stacked2_frames, (small_width, original_height))\n",
    "                stacked3_frames = cv2.resize(stacked3_frames, (small_width, original_height))\n",
    "                stacked4_frames = cv2.resize(stacked4_frames, (small_width, original_height))\n",
    "                stacked5_frames = cv2.resize(stacked5_frames, (small_width, original_height))\n",
    "                \n",
    "                final_layout = np.hstack((frame, stacked1_frames, stacked2_frames, stacked3_frames, stacked4_frames, stacked5_frames))\n",
    "            else:\n",
    "                final_layout = frame\n",
    "\n",
    "            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            image.flags.writeable = False\n",
    "\n",
    "            flat_detected = [landmark for sublist in detected for landmark in sublist]\n",
    "\n",
    "            landmark_list = landmark_pb2.NormalizedLandmarkList()\n",
    "            landmark_list.landmark.extend(flat_detected)\n",
    "            \n",
    "\n",
    "            # Print the elapsed time in seconds since the start of the loop\n",
    "            \n",
    "            fps = time.time() - delay_frame_time\n",
    "            sleep_time = max(0, frame_delay - fps)\n",
    "            \n",
    "            time.sleep(sleep_time)\n",
    "\n",
    "            try:\n",
    "                lv = landmark_list.landmark\n",
    "\n",
    "                counter = 0\n",
    "                featureper_frames = 0\n",
    "                motion_row = []\n",
    "                \n",
    "                for lndmrk in lv:\n",
    "                    motion_row.append(lndmrk.x)\n",
    "                    motion_row.append(lndmrk.y)\n",
    "                    motion_row.append(lndmrk.z)\n",
    "                    motion_row.append(lndmrk.visibility)\n",
    "                    counter += 1\n",
    "\n",
    "                    if counter % 5 == 0:\n",
    "                        motion_row.append(face_direction[featureper_frames])\n",
    "                        motion_row.append(face_shown[featureper_frames])\n",
    "                        motion_row.append(hand_shown[featureper_frames])\n",
    "                        featureper_frames += 1 \n",
    "\n",
    "                motion_row = list(np.array(motion_row))\n",
    "                \n",
    "                motion_detected = pd.DataFrame([motion_row])\n",
    "                motion_class = model.predict(motion_detected)[0]\n",
    "\n",
    "                warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"sklearn\")\n",
    "                cv2.putText(final_layout, f'Class: {motion_class}', (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error : {e}\")\n",
    "                cv2.putText(final_layout, f'Len: {len(motion_row)}', (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "            \n",
    "            # print(f'Class: {motion_class}')\n",
    "\n",
    "            cv2.imshow(\"Camera Capture\", final_layout)\n",
    "\n",
    "            key = cv2.waitKey(1) & 0xFF\n",
    "            if key == ord('q'):\n",
    "                break\n",
    "                \n",
    "camera.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371c08fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "motion_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
